{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-TASK SEQUENCE TO SEQUENCE LEARNING\n",
    "\n",
    "Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, Lukasz Kaiser\n",
    "ICLR 2016\n",
    "\n",
    "https://nlp.stanford.edu/pubs/luong2016iclr_multi.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1. one-to-many\n",
    "1. many-to-one\n",
    "1. many-to-many\n",
    "1. 无监督\n",
    "1. 每一个seq是一个task，每个task先更新一部分参数，task是随机选择的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "In this work, we propose three MTL approaches that complement one another: \n",
    "\n",
    "- (a) the one-to-many approach – for tasks that can have an encoder in common, such as translation and parsing; this applies to the multi-target translation setting in (Dong et al., 2015) as well, \n",
    "- (b) the many-to-one approach – useful for multi-source translation or tasks in which only the decoder can be easily shared, such as translation and image captioning, and lastly, \n",
    "- (c) the many-to-many approach – which share multiple encoders and decoders through which we study the effect of unsupervised learning in translation.\n",
    "\n",
    "## SEQUENCE TO SEQUENCE LEARNING\n",
    "![1](http://ou8qjsj0m.bkt.clouddn.com//17-8-9/97600005.jpg)\n",
    "\n",
    "As illustrated in Figure 1, the encoder computes a representation s for each input sequence. Based on that input representation, the decoder generates an output sequence, one unit at a time, and hence, decomposes the conditional probability as:\n",
    "\n",
    "$$log p(y|x)=\\sum_{j=1}^m log p(y_j|y<j,x,s)\\ (1)$$\n",
    "\n",
    "## MULTI-TASK SEQUENCE-TO-SEQUENCE LEARNING\n",
    "### ONE-TO-MANY SETTING\n",
    "This scheme involves `one encoder` and `multiple decoders` for tasks in which the encoder can be shared, as illustrated in Figure 2.\n",
    "\n",
    "![2](http://ou8qjsj0m.bkt.clouddn.com//17-8-9/83744160.jpg)\n",
    "\n",
    "### MANY-TO-ONE SETTING\n",
    "This scheme is the opposite of the one-to-many setting. As illustrated in Figure 3, it consists of multiple encoders and one decoder.\n",
    "\n",
    "![3](http://ou8qjsj0m.bkt.clouddn.com//17-8-9/26429061.jpg)\n",
    "\n",
    "### MANY-TO-MANY SETTING\n",
    "Lastly, as the name describes, this category is the most general one, consisting of multiple encoders and multiple decoders.\n",
    "\n",
    "![4](http://ou8qjsj0m.bkt.clouddn.com//17-8-9/22808396.jpg)\n",
    "\n",
    "### UNSUPERVISED LEARNING TASKS\n",
    "Skip-thought vectors are trained by training sequence to sequence models on pairs of consecutive sentences, which makes the skip-thought objective a natural seq2seq learning candidate.\n",
    "\n",
    "### LEARNING\n",
    "Dong et al. (2015) adopted an alternating training approach, where they optimize each task for a fixed number of parameter updates (or mini-batches) before switching to the next task (which is a different language pair). In our setting, our tasks are more diverse and contain different amounts of training data. As a result , we allocate different numbers of parameter updates for each task, which are expressed with the mixing ratio values αi (for each task i). Each parameter update consists of training data from one task only. When switching between tasks, we select randomly a new task i \n",
    "with probability $\\frac{\\alpha_i}{\\sum_j \\alpha_j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
