{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Learning with Neural Networks\n",
    "\n",
    "    Ilya Sutskever,Oriol Vinyals,Quoc V. Le\n",
    "    14 Dec 2014\n",
    "\n",
    "https://arxiv.org/pdf/1409.3215.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1. 用一个LSTM读入input sequence，每次输入一个\n",
    "1. 得到一个固定维度的vector\n",
    "1. 用另一个LSTM从vector中输出一个output sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector (fig. 1). \n",
    "\n",
    "![1](http://ou8qjsj0m.bkt.clouddn.com//17-8-8/62828350.jpg)\n",
    "\n",
    "## The model\n",
    "Given a sequence of inputs (x1 , . . . , xT ), a standard RNN computes a sequence of outputs (y1, . . . , yT ) by iterating the following equation:\n",
    "\n",
    "$$h_t=sigm(W^{hx}x_t+W^{hh}h_{t-1}$$\n",
    "\n",
    "$$y_t=W^{yh}h_t$$\n",
    "\n",
    "The goal of the LSTM is to estimate the conditional probability p(y1, . . . , yT ′ |x1, . . . , xT ) where (x1,...,xT)is an input sequence and y1,...,yT′ is its corresponding output sequence whose length T ′ may differ from T . The LSTM computes this conditional probability by first obtaining the fixed-dimensional representation v of the input sequence (x1 , . . . , xT ) given by the last hidden state of the LSTM, and then computing the probability of y1, . . . , yT ′ with a standard LSTM-LM formulation whose initial hidden state is set to the representation v of x1, . . . , xT :\n",
    "\n",
    "$$p(y_1,\\cdots,y_{T'} | x_1,\\cdots,x_T)=\\prod_{t=1}^{T'} p(y_t|v,y_1,\\cdots,y_{t-1})\\ (1)$$\n",
    "\n",
    "In this equation, each p(yt|v, y1, . . . , yt−1) distribution is represented with a softmax over all the words in the vocabulary. \n",
    "\n",
    "Note that we require that each sentence ends with a special end-of-sentence symbol “<EOS>”, which enables the model to define a distribution over sequences of all possible lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
