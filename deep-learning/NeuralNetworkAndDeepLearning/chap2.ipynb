{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the backpropagation algorithm works\n",
    "http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "\n",
    "## Warm up: a fast matrix-based approach to computing the output from a neural network\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz16.png)\n",
    "\n",
    "Explicitly, we use $b^l_j$ for the bias of the $j^{th}$ neuron in the $l^{th}$ layer. And we use $a^l_j$ for the activation of the $j^{th}$ neuron in the $j^{th}$ layer.\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz17.png)\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  a^{l}_j = \\sigma\\left( \\sum_k w^{l}_{jk} a^{l-1}_k + b^l_j \\right),\n",
    "\\end{eqnarray} \\ (23)\n",
    "$\n",
    "\n",
    "where the sum is over all neurons k in the $(l-1)^{th}$ layer.\n",
    "\n",
    "We use the obvious notation σ(v) to denote this kind of elementwise application of a function. That is, the components of σ(v) are just $\\sigma(v)_j = \\sigma(v_j)$. As an example, if we have the function $f(x) = x^2$ then the vectorized form of ff has the effect\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  f\\left(\\left[ \\begin{array}{c} 2 \\\\ 3 \\end{array} \\right] \\right)\n",
    "  = \\left[ \\begin{array}{c} f(2) \\\\ f(3) \\end{array} \\right]\n",
    "  = \\left[ \\begin{array}{c} 4 \\\\ 9 \\end{array} \\right],\n",
    "\\end{eqnarray} \\ (24)\n",
    "$\n",
    "\n",
    "that is, the vectorized f just squares every element of the vector.\n",
    "\n",
    "With these notations in mind, Equation (23)  can be rewritten in the beautiful and compact vectorized form\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  a^{l} = \\sigma(w^l a^{l-1}+b^l).\n",
    "\\end{eqnarray} \\ (25)\n",
    "$\n",
    "\n",
    "When using Equation (25) to compute $a^l$, we compute the intermediate quantity $z^l \\equiv w^l a^{l-1}+b^l$ along the way. We call $z^l$ the weighted input to the neurons in layer l.  Equation (25) is sometimes written in terms of the weighted input, as $a^l =\\sigma(z^l)$.\n",
    "\n",
    "## The two assumptions we need about the cost function\n",
    "In the notation of the last section, the quadratic cost has the form\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  C = \\frac{1}{2n} \\sum_x \\|y(x)-a^L(x)\\|^2,\n",
    "\\end{eqnarray} \\ (26)\n",
    "$\n",
    "\n",
    "- n is the total number of training examples\n",
    "- the sum is over individual training examples, x\n",
    "- y=y(x) is the corresponding desired output\n",
    "- L denotes the number of layers in the network\n",
    "- $a^L = a^L(x)$ is the vector of activations output from the network when x is input\n",
    "\n",
    "The first assumption we need is that the cost function can be written as an average $C = \\frac{1}{n} \\sum_x C_x$ over cost functions $C_x$ for individual training examples, x. The reason we need this assumption is because what backpropagation actually lets us do is compute the partial derivatives $\\partial C_x / \\partial w$ and $\\partial C_x / \\partial b$  for a single training example. We then recover $\\partial C / \\partial w$ and $\\partial C / \\partial b$ by averaging over training examples.\n",
    "\n",
    "The second assumption we make about the cost is that it can be written as a function of the outputs from the neural network:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz18.png)\n",
    "\n",
    "For example, the quadratic cost function satisfies this requirement, since the quadratic cost for a single training example x may be written as\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  C = \\frac{1}{2} \\|y-a^L\\|^2 = \\frac{1}{2} \\sum_j (y_j-a^L_j)^2,\n",
    "\\end{eqnarray} \\ (27)\n",
    "$\n",
    "\n",
    "and thus is a function of the output activations.\n",
    "\n",
    "## The Hadamard product, s⊙t\n",
    "We use s⊙t to denote the elementwise product of the two vectors. Thus the components of s⊙t are just $(s \\odot t)_j = s_j t_j$. As an example,\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\left[\\begin{array}{c} 1 \\\\ 2 \\end{array}\\right] \n",
    "  \\odot \\left[\\begin{array}{c} 3 \\\\ 4\\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 1 * 3 \\\\ 2 * 4 \\end{array} \\right]\n",
    "= \\left[ \\begin{array}{c} 3 \\\\ 8 \\end{array} \\right].\\ (28)\n",
    "\\end{eqnarray} \n",
    "\n",
    "This kind of elementwise multiplication is sometimes called the `Hadamard product` or `Schur product`. \n",
    "\n",
    "## The four fundamental equations behind backpropagation\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz19.png)\n",
    "\n",
    "We define the error $\\delta^l_j$ of neuron j in layer l by\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\delta^l_j \\equiv \\frac{\\partial C}{\\partial z^l_j}. \\ (29)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "An equation for the error in the output layer, $\\delta^L$: The components of $\\delta^L$ are given by\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\delta^L_j = \\frac{\\partial C}{\\partial a^L_j} \\sigma'(z^L_j). \\ (BP1)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "- $\\partial C / \\partial a^L_j$, just measures how fast the cost is changing as a function of the $j^{th}$ output activation.\n",
    "- $\\sigma'(z^L_j)$, measures how fast the activation function $\\sigma$ is changing at $z^L_j$.\n",
    "\n",
    "If we're using the quadratic cost function then $C = \\frac{1}{2} \\sum_j(y_j-a^L_j)^2$, and so $\\partial C / \\partial a^L_j = (a_j^L-y_j)$.\n",
    "\n",
    "It's easy to rewrite the equation in a matrix-based form, as\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\delta^L = \\nabla_a C \\odot \\sigma'(z^L).\\ (BP1a)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "- $\\nabla_a C$ is defined to be a vector whose components are the partial derivatives $\\partial C / \\partial a^L_j$. You can think of $\\nabla_a C$ as expressing the rate of change of C with respect to the output activations.\n",
    "\n",
    "As an example, in the case of the quadratic cost we have $\\nabla_a C =(a^L-y)$, and so the fully matrix-based form of (BP1) becomes\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\delta^L = (a^L-y) \\odot \\sigma'(z^L).\\ (30)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "An equation for the error $\\delta^{l}$ in terms of the error in the next layer, $\\delta^{l+1}$: In particular\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\delta^l = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l),\\ (BP2)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "Suppose we know the error $\\delta^{l+1}$ at the $l+1^{\\rm th}$ layer. When we apply the transpose weight matrix, $(w^{l+1})^T$, we can think intuitively of this as moving the error backward through the network, giving us some sort of measure of the error at the output of the $l^{\\rm th}$ layer. We then take the Hadamard product $\\odot \\sigma'(z^l)$. This moves the error backward through the activation function in layer l, giving us the error $\\delta^l$ in the weighted input to layer l.\n",
    "\n",
    "By combining (BP2) with (BP1) we can compute the error $\\delta^l$ for any layer in the network. We start by using (BP1) to compute $\\delta^L$, then apply Equation (BP2) to compute $\\delta^{L-1}$, then Equation (BP2) again to compute $\\delta^{L-2}$, and so on, all the way back through the network.\n",
    "\n",
    "An equation for the rate of change of the cost with respect to any bias in the network: In particular:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}  \\frac{\\partial C}{\\partial b^l_j} =\n",
    "  \\delta^l_j. \\ (BP3)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "That is, the error $\\delta^l_j$ is exactly equal to the rate of change $\\partial C / \\partial b^l_j$. We can rewrite (BP3) in shorthand as\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial b} = \\delta, \\ (31)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "where it is understood that δ is being evaluated at the same neuron as the bias b.\n",
    "\n",
    "An equation for the rate of change of the cost with respect to any weight in the network: In particular:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j.\\ (BP4)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "This tells us how to compute the partial derivatives $\\partial C / \\partial w^l_{jk}$ in terms of the quantities $\\delta^l$ and $a^{l-1}$, which we already know how to compute. The equation can be rewritten in a less index-heavy notation as\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}  \\frac{\\partial\n",
    "    C}{\\partial w} = a_{\\rm in} \\delta_{\\rm out},\\ (32)\n",
    "\\end{eqnarray}\n",
    "$\n",
    "\n",
    "where it's understood that $a_{in}$ is the activation of the neuron input to the weight w, and $\\delta_{\\rm out}$ is the error of the neuron output from the weight w. Zooming in to look at just the weight w, and the two neurons connected by that weight, we can depict this as:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz20.png)\n",
    "\n",
    "A nice consequence of Equation (32) is that when the activation $a_{in}$ is small, $a_{\\rm in} \\approx 0$, the gradient term $\\partial C / \\partial w$ will also tend to be small. \n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz21.png)\n",
    "\n",
    "## The backpropagation algorithm\n",
    "1. Input x: Set the corresponding activation $a^1$ for the input layer.\n",
    "1. Feedforward: For each $l = 2, 3, \\ldots, L$ compute $z^{l} = w^l a^{l-1}+b^l$ and $a^{l} = \\sigma(z^{l})$.\n",
    "1. Output error $\\delta^L$: Compute the vector $\\delta^{L} = \\nabla_a C \\odot \\sigma'(z^L)$.\n",
    "1. Backpropagate the error: For each $l = L-1, L-2, \\ldots, 2$ compute $\\delta^{l} = ((w^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^{l})$.\n",
    "1. Output: The gradient of the cost function is given by $\\frac{\\partial C}{\\partial w^l_{jk}} = a^{l-1}_k \\delta^l_j$ and $\\frac{\\partial C}{\\partial b^l_j} = \\delta^l_j$.\n",
    "\n",
    "The backpropagation algorithm computes the gradient of the cost function for a single training example, $C = C_x$. In practice, it's common to combine backpropagation with a learning algorithm such as stochastic gradient descent, in which we compute the gradient for many training examples. In particular, given a mini-batch of mm training examples, the following algorithm applies a gradient descent learning step based on that mini-batch:\n",
    "\n",
    "1. Input a set of training examples\n",
    "1. For each training example x: Set the corresponding input activation $a^{x,1}$, and perform the following steps:\n",
    "    1. Feedforward: For each $l = 2, 3, \\ldots, L$ compute $z^{x,l} = w^l a^{x,l-1}+b^l$ and $a^{x,l} = \\sigma(z^{x,l})$.\n",
    "    1. Output error $\\delta^{x,L}$: Compute the vector $\\delta^{x,L} = \\nabla_a C_x \\odot \\sigma'(z^{x,L})$.\n",
    "    1. Backpropagate the error: For each $l = L-1, L-2, \\ldots, 2$ compute $\\delta^{x,l} = ((w^{l+1})^T \\delta^{x,l+1}) \\odot \\sigma'(z^{x,l})$.\n",
    "1. Gradient descent: For each $l = L, L-1, \\ldots, 2$ update the weights according to the rule $w^l \\rightarrow\n",
    "  w^l-\\frac{\\eta}{m} \\sum_x \\delta^{x,l} (a^{x,l-1})^T$, and the biases according to the rule $b^l \\rightarrow b^l-\\frac{\\eta}{m} \\sum_x \\delta^{x,l}$.\n",
    "\n",
    "Of course, to implement stochastic gradient descent in practice you also need an outer loop generating mini-batches of training examples, and an outer loop stepping through multiple epochs of training. I've omitted those for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
