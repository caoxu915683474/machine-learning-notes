{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neural nets to recognize handwritten digits\n",
    "http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "## Perceptrons\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz0.png)\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} & = & \\left\\{ \\begin{array}{ll}\n",
    "      0 & \\mbox{if } \\sum_j w_j x_j \\leq \\mbox{ threshold} \\\\\n",
    "      1 & \\mbox{if } \\sum_j w_j x_j > \\mbox{ threshold}\n",
    "      \\end{array} \\right.\n",
    "\\end{eqnarray} \\ (1)\n",
    "$\n",
    "\n",
    "Using the bias instead of the threshold, the perceptron rule can be rewritten:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\mbox{output} = \\left\\{ \n",
    "    \\begin{array}{ll} \n",
    "      0 & \\mbox{if } w\\cdot x + b \\leq 0 \\\\\n",
    "      1 & \\mbox{if } w\\cdot x + b > 0\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "\\end{eqnarray} \\ (2)\n",
    "$\n",
    "\n",
    "## Sigmoid neurons\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz8.png)\n",
    "\n",
    "Also just like a perceptron, the sigmoid neuron has weights for each input, $w_1, w_2,\\ldots$,and an overall bias, b. But the output is not 0 or 1. Instead, it's $\\sigma(w \\cdot x+b)$, where σ is called the sigmoid function, and is defined by:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}.\n",
    "\\end{eqnarray} \\ (3)\n",
    "$\n",
    "\n",
    "To put it all a little more explicitly, the output of a sigmoid neuron with inputs $x_1,x_2,\\ldots$, weights $w_1,w_2,\\ldots$, and bias b is\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\frac{1}{1+\\exp(-\\sum_j w_j x_j-b)}.\n",
    "\\end{eqnarray} \\ (4)\n",
    "$\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//17-11-5/78296545.jpg)\n",
    "\n",
    "This shape is a smoothed out version of a step function:\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//17-11-5/64005260.jpg)\n",
    "\n",
    "In fact, calculus tells us that Δoutput is well approximated by\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\Delta \\mbox{output} \\approx \\sum_j \\frac{\\partial \\, \\mbox{output}}{\\partial w_j}\n",
    "  \\Delta w_j + \\frac{\\partial \\, \\mbox{output}}{\\partial b} \\Delta b,\n",
    "\\end{eqnarray} \\ (5)\n",
    "$\n",
    "\n",
    "## The architecture of neural networks\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz11.png)\n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "To recognize individual digits we will use a three-layer neural network:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz12.png)\n",
    "\n",
    "Our training data for the network will consist of many 2828 by 2828 pixel images of scanned handwritten digits, and so the input layer contains 784=28×28784=28×28 neurons.\n",
    "\n",
    "We'll experiment with different values for n. The example shown illustrates a small hidden layer, containing just n=15 neurons.\n",
    "\n",
    "## Learning with gradient descent\n",
    "The [MNIST][1] data comes in two parts. The first part contains 60,000 images to be used as training data. The second part of the MNIST data set is 10,000 images to be used as test data.\n",
    "\n",
    "cost function：\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}  C(w,b) \\equiv\n",
    "  \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2.\n",
    "\\end{eqnarray} \\ (6)\n",
    "$\n",
    "\n",
    "[1]: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Calculus tells us that CC changes as follows:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\frac{\\partial C}{\\partial v_1} \\Delta v_1 +\n",
    "  \\frac{\\partial C}{\\partial v_2} \\Delta v_2.\n",
    "\\end{eqnarray} \\ (7)\n",
    "$\n",
    "\n",
    "We denote the gradient vector by ∇C, i.e.:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\nabla C \\equiv \\left( \\frac{\\partial C}{\\partial v_1}, \n",
    "  \\frac{\\partial C}{\\partial v_2} \\right)^T.\n",
    "\\end{eqnarray} \\ (8)\n",
    "$\n",
    "\n",
    "With these definitions, the expression (7) for ΔC can be rewritten as\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\nabla C \\cdot \\Delta v.\n",
    "\\end{eqnarray} \\ (9)\n",
    "$\n",
    "\n",
    "In particular, suppose we choose\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\Delta v = -\\eta \\nabla C,\n",
    "\\end{eqnarray} \\ (10)\n",
    "$\n",
    "\n",
    "where η is a small, positive parameter (known as the `learning rate`). Then Equation (9) tells us that $\\Delta C \\approx -\\eta\n",
    "\\nabla C \\cdot \\nabla C = -\\eta \\|\\nabla C\\|^2$.\n",
    "\n",
    "We'll use Equation (10) to compute a value for Δv, then move the ball's position vv by that amount:\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  v \\rightarrow v' = v -\\eta \\nabla C.\n",
    "\\end{eqnarray} \\ (11)\n",
    "$\n",
    "\n",
    "Then we'll use this update rule again, to make another move. If we keep doing this, over and over, we'll keep decreasing C until - we hope - we reach a global minimum.\n",
    "\n",
    "Suppose in particular that C is a function of m variables, $v_1,\\ldots,v_m$. Then the change ΔC in C produced by a small change $\\Delta v = (\\Delta v_1,\\ldots, \\Delta v_m)^T$ is\n",
    "\n",
    "$\n",
    "\\begin{eqnarray} \n",
    "  \\Delta C \\approx \\nabla C \\cdot \\Delta v,\n",
    "\\end{eqnarray} \\ (12)\n",
    "$\n",
    "\n",
    "where the gradient ∇C is the vector\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\nabla C \\equiv \\left(\\frac{\\partial C}{\\partial v_1}, \\ldots, \n",
    "  \\frac{\\partial C}{\\partial v_m}\\right)^T.\n",
    "\\end{eqnarray} \\ (13)\n",
    "$\n",
    "\n",
    "Just as for the two variable case, we can choose\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  \\Delta v = -\\eta \\nabla C,\n",
    "\\end{eqnarray} \\ (14)\n",
    "$\n",
    "\n",
    "and we're guaranteed that our (approximate) expression (12) for ΔC will be negative. This gives us a way of following the gradient to a minimum, even when C is a function of many variables, by repeatedly applying the update rule\n",
    "\n",
    "$\n",
    "\\begin{eqnarray}\n",
    "  v \\rightarrow v' = v-\\eta \\nabla C.\n",
    "\\end{eqnarray} \\ (15)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
