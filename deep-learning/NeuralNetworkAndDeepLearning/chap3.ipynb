{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the way neural networks learn\n",
    "http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "\n",
    "## The cross-entropy cost function\n",
    "$C = \\frac{(y-a)^2}{2}, \\ (54)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w} = (a-y)\\sigma'(z) x = a \\sigma'(z), \\ (55)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b} = (a-y)\\sigma'(z) = a \\sigma'(z),\\ (56)$\n",
    "\n",
    "where I have substituted x=1 and y=0. \n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//17-11-8/690390.jpg)\n",
    "\n",
    "We can see from this graph that when the neuron's output is close to 1, the curve gets very flat, and so σ′(z) gets very small. Equations (55) and (56) then tell us that ∂C/∂w and ∂C/∂b get very small. This is the origin of the learning slowdown.\n",
    "\n",
    "## Introducing the cross-entropy cost function\n",
    "We'll suppose instead that we're trying to train a neuron with several input variables, x1,x2,… corresponding weights w1,w2,… and a bias, b:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz29.png)\n",
    "\n",
    "$a = \\sigma(z)$\n",
    "\n",
    "$z = \\sum_j w_j x_j+b$\n",
    "\n",
    "The cross-entropy cost function:\n",
    "\n",
    "$C = -\\frac{1}{n} \\sum_x \\left[y \\ln a + (1-y ) \\ln (1-a) \\right], \\ (57)$\n",
    "\n",
    "- n is the total number of items of training data\n",
    "- the sum is over all training inputs, x\n",
    "- y is the corresponding desired output\n",
    "\n",
    "Summing up, the cross-entropy is positive, and tends toward zero as the neuron gets better at computing the desired output, y, for all training inputs, x.\n",
    "\n",
    "We substitute a=σ(z) into (57), and apply the chain rule twice, obtaining:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_j} = -\\frac{1}{n} \\sum_x \\left(\\frac{y}{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\frac{\\partial \\sigma}{\\partial w_j} \\ (58)$\n",
    "\n",
    "$= -\\frac{1}{n} \\sum_x \\left(\\frac{y}{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma'(z) x_j.\\ (59)$\n",
    "\n",
    "Putting everything over a common denominator and simplifying this becomes:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n}\\sum_x \\frac{\\sigma'(z) x_j}{\\sigma(z) (1-\\sigma(z))}(\\sigma(z)-y). \\ (60)$\n",
    "\n",
    "Using the definition of the sigmoid function:\n",
    "\n",
    "$\\sigma(z) = 1/(1+e^{-z})$\n",
    "\n",
    "$\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$\n",
    "\n",
    "It simplifies to become:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w_j} = \\frac{1}{n} \\sum_x x_j(\\sigma(z)-y). \\ (61)$\n",
    "\n",
    "It tells us that the rate at which the weight learns is controlled by σ(z)−y, i.e., by the error in the output. The larger the error, the faster the neuron will learn.\n",
    "\n",
    "In particular, it avoids the learning slowdown caused by the σ′(z) term in the analogous equation for the quadratic cost, Equation (55). When we use the cross-entropy, the σ′(z) term gets canceled out, and we no longer need worry about it being small. \n",
    "\n",
    "In a similar way, we can compute the partial derivative for the bias. \n",
    "\n",
    "$\\frac{\\partial C}{\\partial b} = \\frac{1}{n} \\sum_x (\\sigma(z)-y). \\ (62)$\n",
    "\n",
    "In particular, suppose $y = y_1, y_2, \\ldots$ are the desired values at the output neurons, i.e., the neurons in the final layer, while $a^L_1, a^L_2, \\ldots$ are the actual output values. Then we define the cross-entropy by\n",
    "\n",
    "$C = -\\frac{1}{n} \\sum_x \\sum_j \\left[y_j \\ln a^L_j + (1-y_j) \\ln (1-a^L_j) \\right]. \\ (63)$\n",
    "\n",
    "## Softmax\n",
    "The activation $a^L_j$ of the $j^{th}$ output neuron is\n",
    "\n",
    "$a^L_j = \\frac{e^{z^L_j}}{\\sum_k e^{z^L_k}}, \\ (78)$\n",
    "\n",
    "where in the denominator we sum over all the output neurons.\n",
    "\n",
    "We can prove using Equation (78) and a little algebra:\n",
    "\n",
    "$\\sum_j a^L_j = \\frac{\\sum_j e^{z^L_j}}{\\sum_k e^{z^L_k}} = 1. \\ (79)$\n",
    "\n",
    "The learning slowdown problem:\n",
    "\n",
    "We'll use x to denote a training input to the network, and y to denote the corresponding desired output. Then the log-likelihood cost associated to this training input is\n",
    "\n",
    "$C \\equiv -\\ln a^L_y. \\ (80)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b^L_j} = a^L_j-y_j  \\ (81)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k (a^L_j-y_j) \\ (82)$\n",
    "\n",
    "In fact, it's useful to think of a softmax output layer with log-likelihood cost as being quite similar to a sigmoid output layer with cross-entropy cost.\n",
    "\n",
    "## Overfitting and regularization\n",
    "Let's now look at how the classification accuracy on the test data changes over time:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/overfitting2.png)\n",
    "\n",
    "In the first 200 epochs (not shown) the accuracy rises to just under 82 percent. The learning then gradually slows down. Finally, at around epoch 280 the classification accuracy pretty much stops improving. Later epochs merely see small stochastic fluctuations near the value of the accuracy at epoch 280. We say the network is `overfitting` or overtraining beyond epoch 280.\n",
    "\n",
    "Another sign of overfitting, let's look at the cost on the test data:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/overfitting3.png)\n",
    "\n",
    "We can see that the cost on the test data improves until around epoch 15, but after that it actually starts to get worse, even though the cost on the training data is continuing to get better. \n",
    "\n",
    "Another sign of overfitting may be seen in the classification accuracy on the training data:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/overfitting4.png)\n",
    "\n",
    "The accuracy rises all the way up to 100100 percent. \n",
    "\n",
    "We'll compute the classification accuracy on the `validation_data` at the end of each epoch. Once the classification accuracy on the validation_data has saturated, we stop training. This strategy is called **early stopping**. \n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/overfitting_full.png)\n",
    "\n",
    "As you can see, the accuracy on the test and training data remain much closer together than when we were using 1,000 training examples. In particular, the best classification accuracy of 97.8697.86 percent on the training data is only 2.532.53 percent higher than the 95.3395.33 percent on the test data. That's compared to the 17.7317.73 percent gap we had earlier! **Overfitting is still going on, but it's been greatly reduced.**\n",
    "\n",
    "## Regularization\n",
    "Here's the regularized cross-entropy:\n",
    "\n",
    "$C = -\\frac{1}{n} \\sum_{xj} \\left[y_j \\ln a^L_j+(1-y_j) \\ln(1-a^L_j)\\right] + \\frac{\\lambda}{2n} \\sum_w w^2. \\ (85)$\n",
    "\n",
    "The first term is just the usual expression for the cross-entropy.\n",
    "\n",
    "The second term, namely the sum of the squares of all the weights in the network. This is scaled by a factor λ/2n, where λ>0 is known as the `regularization parameter`, and n is, as usual, the size of our training set.\n",
    "\n",
    "The quadratic cost can be done in a similar way:\n",
    "\n",
    "$C = \\frac{1}{2n} \\sum_x \\|y-a^L\\|^2 + \\frac{\\lambda}{2n} \\sum_w w^2. \\ (86)$\n",
    "\n",
    "In both cases we can write the regularized cost function as\n",
    "\n",
    "$C = C_0 + \\frac{\\lambda}{2n}\\sum_w w^2, \\ (87)$\n",
    "\n",
    "where $C_0$ is the original, unregularized cost function.\n",
    "\n",
    "Taking the partial derivatives of Equation (87) gives\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\ (88)$\n",
    "\n",
    "$\\frac{\\partial C}{\\partial b} = \\frac{\\partial C_0}{\\partial b}.\\ (89)$\n",
    "\n",
    "The gradient descent learning rule for the biases doesn't change from the usual rule:\n",
    "\n",
    "$b \\rightarrow b -\\eta \\frac{\\partial C_0}{\\partial b}.\\ (90)$\n",
    "\n",
    "The learning rule for the weights becomes:\n",
    "\n",
    "$w \\rightarrow w-\\eta \\frac{\\partial C_0}{\\partial w}-\\frac{\\eta \\lambda}{n} w \\ (91)$\n",
    "\n",
    "$= \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial C_0}{\\partial w}. \\ (92)$\n",
    "\n",
    "This is exactly the same as the usual gradient descent learning rule, except we first rescale the weight w by a factor $1-\\frac{\\eta\\lambda}{n}$. This rescaling is sometimes referred to as `weight decay`, since it makes the weights smaller.\n",
    "\n",
    "The regularized learning rule for stochastic gradient descent becomes:\n",
    "\n",
    "$w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial w}, \\ (93)$\n",
    "\n",
    "$b \\rightarrow b - \\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial b},\\ (94)$\n",
    "\n",
    "where the sum is over training examples x in the mini-batch, and $C_x$ is the (unregularized) cost for each training example.\n",
    "\n",
    "## Why does regularization help reduce overfitting?\n",
    "A standard story people tell to explain what's going on is along the following lines: smaller weights are, in some sense, lower complexity, and so **provide a simpler and more powerful explanation for the data, and should thus be preferred.** That's a pretty terse story, though, and contains several elements that perhaps seem dubious or mystifying.\n",
    "\n",
    "The fact that L2 regularization doesn't constrain the biases.\n",
    "\n",
    "## Other techniques for regularization\n",
    "I briefly describe three other approaches to reducing overfitting: L1 regularization, dropout, and artificially increasing the training set size. \n",
    "\n",
    "**L1 regularization**:\n",
    "\n",
    "$C = C_0 + \\frac{\\lambda}{n} \\sum_w |w|.\\ (95)$\n",
    "\n",
    "We'll look at the partial derivatives of the cost function. Differentiating (95) we obtain:\n",
    "\n",
    "$\\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} {sgn}(w),\\ (96)$\n",
    "\n",
    "- sgn(w) is the sign of w\n",
    "\n",
    "The resulting update rule for an L1 regularized network is\n",
    "\n",
    "$w \\rightarrow w' = w-\\frac{\\eta \\lambda}{n} {sgn}(w) - \\eta \\frac{\\partial C_0}{\\partial w}, \\ (97)$\n",
    "\n",
    "Compare that to the update rule for L2 regularization (c.f. Equation (93)),\n",
    "\n",
    "$w \\rightarrow w' = w\\left(1 - \\frac{\\eta \\lambda}{n} \\right) - \\eta \\frac{\\partial C_0}{\\partial w}. \\ (98)$\n",
    "\n",
    "In both expressions the effect of regularization is to shrink the weights. This accords with our intuition that both kinds of regularization penalize large weights. But the way the weights shrink is different. In L1 regularization, the weights shrink by a constant amount toward 0. In L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does. By contrast, when |w| is small, L1 regularization shrinks the weight much more than L2 regularization. The net result is that L1 regularization tends to concentrate the weight of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.\n",
    "\n",
    "**Dropout**:\n",
    "\n",
    "Suppose we're trying to train a network:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz30.png)\n",
    "\n",
    "We start by randomly (and temporarily) deleting half the hidden neurons in the network, while leaving the input and output neurons untouched. After doing this, we'll end up with a network along the following lines. Note that the dropout neurons, i.e., the neurons which have been temporarily deleted, are still ghosted in:\n",
    "\n",
    "![](http://neuralnetworksanddeeplearning.com/images/tikz31.png)\n",
    "\n",
    "We forward-propagate the input xx through the modified network, and then backpropagate the result, also through the modified network. After doing this over a mini-batch of examples, we update the appropriate weights and biases. We then repeat the process, first restoring the dropout neurons, then choosing a new random subset of hidden neurons to delete, estimating the gradient for a different mini-batch, and updating the weights and biases in the network.\n",
    "\n",
    "By repeating this process over and over, our network will learn a set of weights and biases. Of course, those weights and biases will have been learnt under conditions in which half the hidden neurons were dropped out. When we actually run the full network that means that twice as many hidden neurons will be active. \n",
    "\n",
    "**The reason is that the different networks may overfit in different ways, and averaging may help eliminate that kind of overfitting.**\n",
    "\n",
    "## Handwriting recognition revisited: the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training complete\n",
      "Cost on training data: 0.478850757921\n",
      "Accuracy on training data: 47053 / 50000\n",
      "Cost on evaluation data: 0.780147817261\n",
      "Accuracy on evaluation data: 9425 / 10000\n",
      "\n",
      "Epoch 1 training complete\n",
      "Cost on training data: 0.443697058681\n",
      "Accuracy on training data: 47465 / 50000\n",
      "Cost on evaluation data: 0.84408265512\n",
      "Accuracy on evaluation data: 9494 / 10000\n",
      "\n",
      "Epoch 2 training complete\n",
      "Cost on training data: 0.436786090824\n",
      "Accuracy on training data: 47677 / 50000\n",
      "Cost on evaluation data: 0.890229406699\n",
      "Accuracy on evaluation data: 9519 / 10000\n",
      "\n",
      "Epoch 3 training complete\n",
      "Cost on training data: 0.407652260249\n",
      "Accuracy on training data: 48001 / 50000\n",
      "Cost on evaluation data: 0.892164701353\n",
      "Accuracy on evaluation data: 9566 / 10000\n",
      "\n",
      "Epoch 4 training complete\n",
      "Cost on training data: 0.399290825757\n",
      "Accuracy on training data: 47998 / 50000\n",
      "Cost on evaluation data: 0.902849979457\n",
      "Accuracy on evaluation data: 9577 / 10000\n",
      "\n",
      "Epoch 5 training complete\n",
      "Cost on training data: 0.394505528381\n",
      "Accuracy on training data: 48131 / 50000\n",
      "Cost on evaluation data: 0.90405773553\n",
      "Accuracy on evaluation data: 9597 / 10000\n",
      "\n",
      "Epoch 6 training complete\n",
      "Cost on training data: 0.449922346948\n",
      "Accuracy on training data: 47792 / 50000\n",
      "Cost on evaluation data: 0.982141726881\n",
      "Accuracy on evaluation data: 9526 / 10000\n",
      "\n",
      "Epoch 7 training complete\n",
      "Cost on training data: 0.378149237342\n",
      "Accuracy on training data: 48323 / 50000\n",
      "Cost on evaluation data: 0.924016802886\n",
      "Accuracy on evaluation data: 9615 / 10000\n",
      "\n",
      "Epoch 8 training complete\n",
      "Cost on training data: 0.386170612862\n",
      "Accuracy on training data: 48285 / 50000\n",
      "Cost on evaluation data: 0.935845631148\n",
      "Accuracy on evaluation data: 9589 / 10000\n",
      "\n",
      "Epoch 9 training complete\n",
      "Cost on training data: 0.375592786604\n",
      "Accuracy on training data: 48414 / 50000\n",
      "Cost on evaluation data: 0.9271327944\n",
      "Accuracy on evaluation data: 9625 / 10000\n",
      "\n",
      "Epoch 10 training complete\n",
      "Cost on training data: 0.389331966056\n",
      "Accuracy on training data: 48226 / 50000\n",
      "Cost on evaluation data: 0.952710791882\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "\n",
      "Epoch 11 training complete\n",
      "Cost on training data: 0.385824849324\n",
      "Accuracy on training data: 48217 / 50000\n",
      "Cost on evaluation data: 0.941983819562\n",
      "Accuracy on evaluation data: 9602 / 10000\n",
      "\n",
      "Epoch 12 training complete\n",
      "Cost on training data: 0.3951198237\n",
      "Accuracy on training data: 48195 / 50000\n",
      "Cost on evaluation data: 0.954332451025\n",
      "Accuracy on evaluation data: 9582 / 10000\n",
      "\n",
      "Epoch 13 training complete\n",
      "Cost on training data: 0.383482569178\n",
      "Accuracy on training data: 48308 / 50000\n",
      "Cost on evaluation data: 0.94235134253\n",
      "Accuracy on evaluation data: 9619 / 10000\n",
      "\n",
      "Epoch 14 training complete\n",
      "Cost on training data: 0.434237947836\n",
      "Accuracy on training data: 48016 / 50000\n",
      "Cost on evaluation data: 1.00121660459\n",
      "Accuracy on evaluation data: 9549 / 10000\n",
      "\n",
      "Epoch 15 training complete\n",
      "Cost on training data: 0.394137821592\n",
      "Accuracy on training data: 48266 / 50000\n",
      "Cost on evaluation data: 0.954406284313\n",
      "Accuracy on evaluation data: 9601 / 10000\n",
      "\n",
      "Epoch 16 training complete\n",
      "Cost on training data: 0.37638205918\n",
      "Accuracy on training data: 48342 / 50000\n",
      "Cost on evaluation data: 0.935203605093\n",
      "Accuracy on evaluation data: 9637 / 10000\n",
      "\n",
      "Epoch 17 training complete\n",
      "Cost on training data: 0.373964618618\n",
      "Accuracy on training data: 48429 / 50000\n",
      "Cost on evaluation data: 0.933349751145\n",
      "Accuracy on evaluation data: 9636 / 10000\n",
      "\n",
      "Epoch 18 training complete\n",
      "Cost on training data: 0.363200594548\n",
      "Accuracy on training data: 48495 / 50000\n",
      "Cost on evaluation data: 0.93306141544\n",
      "Accuracy on evaluation data: 9635 / 10000\n",
      "\n",
      "Epoch 19 training complete\n",
      "Cost on training data: 0.389829639652\n",
      "Accuracy on training data: 48317 / 50000\n",
      "Cost on evaluation data: 0.962513192296\n",
      "Accuracy on evaluation data: 9595 / 10000\n",
      "\n",
      "Epoch 20 training complete\n",
      "Cost on training data: 0.375912926731\n",
      "Accuracy on training data: 48431 / 50000\n",
      "Cost on evaluation data: 0.945813226393\n",
      "Accuracy on evaluation data: 9622 / 10000\n",
      "\n",
      "Epoch 21 training complete\n",
      "Cost on training data: 0.381774797831\n",
      "Accuracy on training data: 48370 / 50000\n",
      "Cost on evaluation data: 0.941339428308\n",
      "Accuracy on evaluation data: 9629 / 10000\n",
      "\n",
      "Epoch 22 training complete\n",
      "Cost on training data: 0.372120359323\n",
      "Accuracy on training data: 48444 / 50000\n",
      "Cost on evaluation data: 0.945707322009\n",
      "Accuracy on evaluation data: 9625 / 10000\n",
      "\n",
      "Epoch 23 training complete\n",
      "Cost on training data: 0.410444437911\n",
      "Accuracy on training data: 48178 / 50000\n",
      "Cost on evaluation data: 0.988274002713\n",
      "Accuracy on evaluation data: 9566 / 10000\n",
      "\n",
      "Epoch 24 training complete\n",
      "Cost on training data: 0.383057367776\n",
      "Accuracy on training data: 48276 / 50000\n",
      "Cost on evaluation data: 0.957008854022\n",
      "Accuracy on evaluation data: 9578 / 10000\n",
      "\n",
      "Epoch 25 training complete\n",
      "Cost on training data: 0.359598256298\n",
      "Accuracy on training data: 48501 / 50000\n",
      "Cost on evaluation data: 0.936284860066\n",
      "Accuracy on evaluation data: 9637 / 10000\n",
      "\n",
      "Epoch 26 training complete\n",
      "Cost on training data: 0.380776625031\n",
      "Accuracy on training data: 48392 / 50000\n",
      "Cost on evaluation data: 0.954314969209\n",
      "Accuracy on evaluation data: 9609 / 10000\n",
      "\n",
      "Epoch 27 training complete\n",
      "Cost on training data: 0.367621123865\n",
      "Accuracy on training data: 48471 / 50000\n",
      "Cost on evaluation data: 0.93427601622\n",
      "Accuracy on evaluation data: 9634 / 10000\n",
      "\n",
      "Epoch 28 training complete\n",
      "Cost on training data: 0.382718937975\n",
      "Accuracy on training data: 48310 / 50000\n",
      "Cost on evaluation data: 0.94720399465\n",
      "Accuracy on evaluation data: 9614 / 10000\n",
      "\n",
      "Epoch 29 training complete\n",
      "Cost on training data: 0.370930187031\n",
      "Accuracy on training data: 48435 / 50000\n",
      "Cost on evaluation data: 0.938415398939\n",
      "Accuracy on evaluation data: 9620 / 10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.78014781726142135,\n",
       "  0.84408265511979197,\n",
       "  0.89022940669924955,\n",
       "  0.89216470135278947,\n",
       "  0.90284997945730083,\n",
       "  0.90405773553029234,\n",
       "  0.98214172688143564,\n",
       "  0.92401680288569232,\n",
       "  0.93584563114802966,\n",
       "  0.92713279440004093,\n",
       "  0.95271079188215801,\n",
       "  0.9419838195623853,\n",
       "  0.95433245102496667,\n",
       "  0.94235134253021169,\n",
       "  1.0012166045938655,\n",
       "  0.95440628431329322,\n",
       "  0.93520360509257461,\n",
       "  0.9333497511445531,\n",
       "  0.93306141544043952,\n",
       "  0.9625131922963579,\n",
       "  0.94581322639251653,\n",
       "  0.94133942830841644,\n",
       "  0.94570732200907814,\n",
       "  0.98827400271308619,\n",
       "  0.95700885402158575,\n",
       "  0.93628486006556444,\n",
       "  0.9543149692086571,\n",
       "  0.9342760162199828,\n",
       "  0.9472039946502453,\n",
       "  0.93841539893931469],\n",
       " [9425,\n",
       "  9494,\n",
       "  9519,\n",
       "  9566,\n",
       "  9577,\n",
       "  9597,\n",
       "  9526,\n",
       "  9615,\n",
       "  9589,\n",
       "  9625,\n",
       "  9574,\n",
       "  9602,\n",
       "  9582,\n",
       "  9619,\n",
       "  9549,\n",
       "  9601,\n",
       "  9637,\n",
       "  9636,\n",
       "  9635,\n",
       "  9595,\n",
       "  9622,\n",
       "  9629,\n",
       "  9625,\n",
       "  9566,\n",
       "  9578,\n",
       "  9637,\n",
       "  9609,\n",
       "  9634,\n",
       "  9614,\n",
       "  9620],\n",
       " [0.47885075792071663,\n",
       "  0.44369705868077208,\n",
       "  0.43678609082354852,\n",
       "  0.40765226024901752,\n",
       "  0.39929082575687469,\n",
       "  0.39450552838109959,\n",
       "  0.44992234694815941,\n",
       "  0.3781492373419868,\n",
       "  0.38617061286246979,\n",
       "  0.37559278660445461,\n",
       "  0.38933196605556353,\n",
       "  0.38582484932441186,\n",
       "  0.39511982370036458,\n",
       "  0.38348256917786516,\n",
       "  0.43423794783608305,\n",
       "  0.39413782159224103,\n",
       "  0.37638205918020107,\n",
       "  0.37396461861765951,\n",
       "  0.36320059454803033,\n",
       "  0.38982963965193185,\n",
       "  0.37591292673127108,\n",
       "  0.38177479783061685,\n",
       "  0.37212035932307275,\n",
       "  0.41044443791099722,\n",
       "  0.38305736777613247,\n",
       "  0.35959825629846753,\n",
       "  0.38077662503087184,\n",
       "  0.36762112386466317,\n",
       "  0.38271893797508083,\n",
       "  0.37093018703131064],\n",
       " [47053,\n",
       "  47465,\n",
       "  47677,\n",
       "  48001,\n",
       "  47998,\n",
       "  48131,\n",
       "  47792,\n",
       "  48323,\n",
       "  48285,\n",
       "  48414,\n",
       "  48226,\n",
       "  48217,\n",
       "  48195,\n",
       "  48308,\n",
       "  48016,\n",
       "  48266,\n",
       "  48342,\n",
       "  48429,\n",
       "  48495,\n",
       "  48317,\n",
       "  48431,\n",
       "  48370,\n",
       "  48444,\n",
       "  48178,\n",
       "  48276,\n",
       "  48501,\n",
       "  48392,\n",
       "  48471,\n",
       "  48310,\n",
       "  48435])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import network2\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.SGD(training_data, 30, 10, 0.5,\n",
    "        lmbda = 5.0,\n",
    "        evaluation_data=validation_data,\n",
    "        monitor_evaluation_accuracy=True,\n",
    "        monitor_evaluation_cost=True,\n",
    "        monitor_training_accuracy=True,\n",
    "         monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
