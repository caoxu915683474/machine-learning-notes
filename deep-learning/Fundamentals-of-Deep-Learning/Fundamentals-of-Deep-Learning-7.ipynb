{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning\n",
    "## 目录\n",
    "- Chapter 7. Models for Sequence Analysis\n",
    "    - Analyzing Variable-Length Inputs\n",
    "    - Tackling seq2seq with Neural N-Grams\n",
    "    - Implementing a Part-of-Speech Tagger\n",
    "    - Dependency Parsing and SyntaxNet\n",
    "    - Beam Search and Global Normalization\n",
    "    - A Case for Stateful Deep Learning Models\n",
    "    - Recurrent Neural Networks\n",
    "    - Long Short-Term Memory (LSTM) Units\n",
    "\n",
    "## Analyzing Variable-Length Inputs\n",
    "In Figure 7-1, we illustrate how our feed-forward neural networks break when analyzing sequences. If the sequence is the same size as the input layer, the model can perform as we expect it to. It’s even possible to deal with smaller inputs by **padding zeros to the end of the input until it’s the appropriate length**. However, the moment the input exceeds the size of the input layer, naively using the feedforward network no longer works.\n",
    "\n",
    "![7-1](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0701.png)\n",
    "\n",
    "Figure 7-1. Feed-forward networks thrive on fixed input size problems. Zero padding can address the handling of smaller inputs, but when naively utilized, these models break when inputs exceed the fixed input size. \n",
    "\n",
    "## Tackling seq2seq with Neural N-Grams\n",
    "In this section, we’ll begin exploring a feed-forward neural network architecture that can process a body of text and produce a sequence of `part-of-speech (POS)` tags. An example of this is shown in Figure 7-2. \n",
    "\n",
    "![7-2](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0702.png)\n",
    "\n",
    "Figure 7-2. An example of an accurate POS parse of an English sentence\n",
    "\n",
    "We can predict each POS tag one at a time by using a fixed-length subsequence. In particular, we utilize the subsequence starting from the word of interest and extending n words into the past. This neural n-gram strategy is depicted in Figure 7-3.\n",
    "\n",
    "![7-3](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/assets/fodl_0703.png)\n",
    "\n",
    "Figure 7-3. Using a feed-forward network to perform seq2seq when we can ignore long-term dependencies\n",
    "\n",
    "Specifically, when we predict the POS tag for the $i^{th}$ word in the input, we utilize the $i - n + 1^{st}, i - n + 2^{nd}, \\cdots, i^{th}$ words as the input. We’ll refer to this subsequence as the `context window`.\n",
    "\n",
    "## Implementing a Part-of-Speech Tagger\n",
    "On a high level, the network consists of an input layer that leverages a 3-gram context window. We’ll utilize word embeddings that are 300-dimensional, resulting in a context window of size 900. The feed-forward network will have two hidden layers of size 512 neurons and 256 neurons, respectively. Finally, the output layer will be a softmax calculating the probability distribution of the POS tag output over a space of 44 possible tags.\n",
    "\n",
    "The tricky part of building the POS tagger is in preparing the dataset. We’ll leverage pretrained word embeddings generated from Google News. It includes vectors for 3 million words and phrases and was trained on roughly 100 billion words. \n",
    "\n",
    "As we mentioned, the gensim model contains three million words, which is larger than our dataset. For the sake of efficiency, we’ll selectively cache word vectors for words in our dataset and discard everything else. To figure out which words we’d like to cache, let’s download the POS dataset from the CoNLL-2000 task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence NN\r\n",
      "in IN\r\n",
      "the DT\r\n",
      "pound NN\r\n",
      "is VBZ\r\n",
      "widely RB\r\n",
      "expected VBN\r\n",
      "to TO\r\n",
      "take VB\r\n",
      "another DT\r\n"
     ]
    }
   ],
   "source": [
    "!head data/pos.train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rockwell NNP\r\n",
      "International NNP\r\n",
      "Corp. NNP\r\n",
      "'s POS\r\n",
      "Tulsa NNP\r\n",
      "unit NN\r\n",
      "said VBD\r\n",
      "it PRP\r\n",
      "signed VBD\r\n",
      "a DT\r\n"
     ]
    }
   ],
   "source": [
    "!head data/pos.test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
