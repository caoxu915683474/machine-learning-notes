# Fundamentals of Deep Learning
![cover](https://img3.doubanio.com/lpic/s29474106.jpg)

    作者: Nikhil Buduma 
    出版社: O'Reilly Media
    副标题: Designing Next-Generation Machine Intelligence Algorithms
    出版年: 2017-6-29
    页数: 304
    定价: USD 43.99
    装帧: Paperback
    ISBN: 9781491925614

- [豆瓣链接](https://book.douban.com/subject/26425877/)
- [safaribooks](https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/)

## 目录
- [Chapter 1. The Neural Network][1]
    - The Neuron
    - Feed-Forward Neural Networks
    - Linear Neurons
    - Sigmoid, Tanh, and ReLU Neurons
- [Chapter 2. Training Feed-Forward Neural Networks][1]
    - The Fast-Food Problem
    - Gradient Descent
    - The Delta Rule and Learning Rates
    - Gradient Descent with Sigmoidal Neurons
    - The Backpropagation Algorithm
    - Stochastic and Minibatch Gradient Descent
    - Test Sets, Validation Sets, and Overfitting
    - Preventing Overfitting in Deep Neural Networks
- [Chapter 3. Implementing Neural Networks in TensorFlow][2]
    - Installing TensorFlow
    - Creating and Manipulating TensorFlow Variables
    - TensorFlow Operations
    - Placeholder Tensors
    - Sessions in TensorFlow
    - Navigating Variable Scopes and Sharing Variables
    - Managing Models over the CPU and GPU
    - Specifying the Logistic Regression Model in TensorFlow
    - Logging and Training the Logistic Regression Model
    - Leveraging TensorBoard to Visualize Computation Graphs and Learning
    - Building a Multilayer Model for MNIST in TensorFlow
- [Chapter 4. Beyond Gradient Descent][3]
    - Local Minima in the Error Surfaces of Deep Networks
    - How Pesky Are Spurious Local Minima in Deep Networks?
    - Flat Regions in the Error Surface
    - When the Gradient Points in the Wrong Direction
    - Momentum-Based Optimization
    - A Brief View of Second-Order Methods
        - Conjugate Gradient Descent
        - Broyden–Fletcher–Goldfarb–Shanno (BFGS)
    - Learning Rate Adaptation
        - AdaGrad—Accumulating Historical Gradients
        - RMSProp—Exponentially Weighted Moving Average of Gradients
        - Adam—Combining Momentum and RMSProp
    - Optimization Algorithms Experiment
- [Chapter 5. Convolutional Neural Networks][4]
    - Vanilla Deep Neural Networks Don’t Scale
    - Filters and Feature Maps
    - Full Description of the Convolutional Layer
    - Max Pooling
    - Full Architectural Description of Convolution Networks
    - Closing the Loop on MNIST with Convolutional Networks
    - Building a Convolutional Network for CIFAR-10
    - Visualizing Learning in Convolutional Networks
    - Leveraging Convolutional Filters to Replicate Artistic Styles

[1]: Fundamentals-of-Deep-Learning-1+2.ipynb
[2]: Fundamentals-of-Deep-Learning-3.ipynb
[3]: Fundamentals-of-Deep-Learning-4.ipynb
[4]: Fundamentals-of-Deep-Learning-5.ipynb
