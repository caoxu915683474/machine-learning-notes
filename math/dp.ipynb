{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 狄利克雷过程（dirichlet process ）的五种理解\n",
    "狄利克雷过程（dirichlet process ）是目前变参数学习（non parameter）非常流行的一个理论，很多的工作都是基于这个理论来进行的，如HDP（hierarchical dirichlet process）。\n",
    "\n",
    "下面我们谈谈dirichlet process的五种角度来理解它。\n",
    "\n",
    "## 第一种：原始定义\n",
    "假设存在在度量空间$\\Theta$上的分布H和一个参数$\\alpha$，如果对于度量空间$\\Theta$的任意一个可数划分（可以是有限或者无限的）A1, A2,...,An，都有下列式子成立：\n",
    "\n",
    "$(G(A1),G(A2),...,G(An)) \\sim Dir(\\alpha H(A1), \\alpha H(A2),..., \\alpha H(An))$,  这里Dir是dirichlet 分布，\n",
    "\n",
    "我们称G是满足Dirichlet process的。\n",
    "\n",
    "这个定义是1973年Ferguson最早提出的定义。在有了这个定义之后，我们怎么去构造一个dirichlet process（DP）出来呢？或者如果我们想从这个DP中抽取出一些样本，怎么抽呢？由于这个原因，我们有了下面三种构造性定义或者解释： 中国餐馆过程（CRP)，polya urn ，stick-breaking。\n",
    "\n",
    "## 第二种：中国餐馆过程（CRP）\n",
    "假设一个中国餐馆有无限的桌子，第一个顾客到来之后坐在第一张桌子上。第二个顾客来到可以选择坐在第一张桌子上，也可以选择坐在一张新的桌子上，假设第n＋1个顾客到来的时候，已经有k张桌子上有顾客了，分别坐了n1,n2,...，nk个顾客，那么第n＋1个顾客可以以概率为$\\frac{n_i}{\\alpha+n}$坐在第i张桌子上，ni为第i张桌子上的顾客数；同时有概率为$\\frac{\\alpha}{\\alpha+n}$选取一张新的桌子坐下。那么在n个顾客坐定之后，很显然CRP把这n个顾客分为了K个堆，即K个clusters，可以证明CRP就是一个DP。\n",
    "\n",
    "注意这里有一个限制，每张桌子上只能有同一个dish，即一桌人喜欢吃同一道菜。\n",
    "\n",
    "## 第三种：Polya urn模型\n",
    "假设我们有一个缸，里面没有球，现在我们从一个分布H中选取一种颜色，然后把这种颜色涂在一个球上放入缸中；然后我们要么从缸中抽取一个球出来，然后再放入两个和这个球同种颜色的球进入缸中；要么就从分布H中选取一个颜色，然后把这种颜色涂在一个球上放入缸中。从缸中抽取某种颜色的一个球的概率是$\\frac{n_i}{\\alpha+n}$，ni是这种颜色的球的个数，n是总的球个数；不从缸中抽取而放入一种颜色的球的概率是$\\frac{\\alpha}{\\alpha+n}$。很明显，polya urn模型和CRP有一一对应的关系，颜色对应一个桌子，坐新桌子对应于不从缸中选取而是从H中选取一种颜色涂球放入缸中。\n",
    "\n",
    "## 第四种：stick-breaking模型\n",
    "假设有一个长度为1的线段，我们从中选取$\\pi_1$长出来，剩下的部分再选取$\\pi_2$出来，循环下去，$\\pi_n$，无穷下去，这个有点类似我们古代的一句话：\n",
    "\n",
    "“一尺之踵，日取其半，万世不竭”，它们满足$\\sum \\pi_i=1$\n",
    "\n",
    "对每个$\\pi_i$，我们都从分布H中选取一个$\\theta_i$，然后从$F(\\theta_i)$中选取出一个$x_i$出来。这里的$\\theta_i$就对应一个cluster，类似地，我们可以看到数据自然地被分为了各个堆，可以证明这个模型仍然是一个DP。\n",
    "\n",
    "## 第五种：无限混合模型\n",
    "从stick－breaking模型我们看出，我们可以把DP看着是一个无限混合模型，即\n",
    "\n",
    "$G \\sim \\sum_1^\\infty \\pi_i * F(\\theta_i)$，其中$\\sum\\pi_i=1$。$\\pi_i$ 就是混合模型中每个子模型的权重。\n",
    "\n",
    "目前应用最多的还是从第五种角度来看待问题，即把DP看着是一个无限混合模型，其中值得注意的是：\n",
    "\n",
    "1. 虽然DP是一个无限混合模型，但是可以证明，随着数据的增多，模型的个数是呈现log 增加的，即模型的个数的增长是比数据的增长要缓慢得多的；\n",
    "1. DP是有一个马太效应在里面的，即越富裕的人越来越富裕，我们可以从第二和第三种解释中看到，每个桌子或者颜色已经有的数据越多，那么下一次被选中的概率越大，因为是与在桌子上的个数成正比的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
