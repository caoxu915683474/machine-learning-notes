{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC -->\n",
    "\n",
    "- [神奇的 Gamma 函数](#神奇的-gamma-函数)\n",
    "    - [1. 神奇的 Gamma 函数](#1-神奇的-gamma-函数)\n",
    "        - [1.1 Gamma 函数诞生记](#11-gamma-函数诞生记)\n",
    "        - [1.2 Gamma 函数欣赏](#12-gamma-函数欣赏)\n",
    "            - [Stirling 公式](#stirling-公式)\n",
    "            - [复平面上的 Gamma 函数](#复平面上的-gamma-函数)\n",
    "            - [1/2 阶导数](#12-阶导数)\n",
    "            - [欧拉常数γ](#欧拉常数γ)\n",
    "            - [黎曼函数ζ(s)](#黎曼函数ζs)\n",
    "            - [logΓ(x)](#logγx)\n",
    "        - [1.3 从二项分布到 Gamma 分布](#13-从二项分布到-gamma-分布)\n",
    "- [认识 Beta/Dirichlet 分布](#认识-betadirichlet-分布)\n",
    "    - [2. 认识 Beta/Dirichlet 分布](#2-认识-betadirichlet-分布)\n",
    "        - [2.1 魔鬼的游戏—认识 Beta 分布](#21-魔鬼的游戏认识-beta-分布)\n",
    "        - [2.2 Beta-Binomial 共轭](#22-beta-binomial-共轭)\n",
    "        - [2.3 Dirichlet-Multinomial 共轭](#23-dirichlet-multinomial-共轭)\n",
    "        - [2.4 Beta/Dirichlet 分布的一个性质](#24-betadirichlet-分布的一个性质)\n",
    "- [MCMC 和 Gibbs Sampling](#mcmc-和-gibbs-sampling)\n",
    "    - [MCMC 和 Gibbs Sampling](#mcmc-和-gibbs-sampling-1)\n",
    "        - [3.1 随机模拟](#31-随机模拟)\n",
    "            - [蒙特卡罗方法](#蒙特卡罗方法)\n",
    "            - [生成一个概率分布的样本](#生成一个概率分布的样本)\n",
    "        - [3.2 马氏链及其平稳分布](#32-马氏链及其平稳分布)\n",
    "        - [3.3 Markov Chain Monte Carlo](#33-markov-chain-monte-carlo)\n",
    "        - [3.4 Gibbs Sampling](#34-gibbs-sampling)\n",
    "- [参考资料](#参考资料)\n",
    "\n",
    "<!-- /TOC -->\n",
    "\n",
    "\n",
    "# 神奇的 Gamma 函数\n",
    "## 1. 神奇的 Gamma 函数\n",
    "### 1.1 Gamma 函数诞生记\n",
    "Gamma 函数：\n",
    "\n",
    "$\\Gamma(x)=\\int_{0}^{\\infty} t^{x-1}e^{-t}df$\n",
    "\n",
    "![1](https://uploads.cosx.org/2013/01/gamma-func.png)\n",
    "\n",
    "$\\Gamma(x+1)=x\\Gamma(x)$\n",
    "\n",
    "$\\Gamma(n)=(n-1)!$\n",
    "\n",
    "### 1.2 Gamma 函数欣赏\n",
    "#### Stirling 公式\n",
    "Gamma 函数作为阶乘的推广，首先它也有和 Stirling 公式类似的一个结论:\n",
    "\n",
    "$\\Gamma(x) \\sim \\sqrt{2\\pi} e^{-x}x^{x-\\frac{1}{2}}$\n",
    "\n",
    "#### 复平面上的 Gamma 函数\n",
    "Gamma 函数不仅可以定义在实数集上，还可以延拓到整个复平面上:\n",
    "\n",
    "![2](https://uploads.cosx.org/2013/01/gamma-complex.png)\n",
    "\n",
    "#### 1/2 阶导数\n",
    "Gamma 函数有很多妙用，它不但使得 (1⁄2)! 的计算有意义。有了 Gamma 函数我们可以把函数导数的定义延拓到实数集，从而可以计算 1/2 阶导数。我们先考虑一下 \n",
    "$x^n$的各阶导数\n",
    "\n",
    "![3](https://uploads.cosx.org/2013/01/derivatives.png)\n",
    "\n",
    "由于 k 阶导数可以用阶乘表达，于是我们用 Gamma 函数表达为\n",
    "\n",
    "$\\frac{\\Gamma(n+1)}{\\Gamma(n-k+1)}x^{n-k}$\n",
    "\n",
    "于是基于上式，我们可以把导数的阶从整数延拓到实数集。例如，取 n=1,k=1/2, 我们可以计算 x的1/2 阶导数为\n",
    "\n",
    "$\\frac{\\Gamma(n+1)}{\\Gamma(n-k+1)}x^{1-1/2}=\\frac{2\\sqrt{x}}{\\sqrt{\\pi}}$\n",
    "\n",
    "#### 欧拉常数γ\n",
    "$\\gamma=-\\frac{d\\Gamma(x)}{dx}|_{x=1}=lim_{n \\to \\infty}(1+\\frac{1}{2}+\\frac{1}{3}+ ... + \\frac{1}{n}-logn)$\n",
    "\n",
    "#### 黎曼函数ζ(s)  \n",
    "$\\zeta(s)=1+\\frac{1}{2^s}+\\frac{1}{3^s}+...$\n",
    "\n",
    "#### logΓ(x)\n",
    "![4](https://uploads.cosx.org/2013/01/digamma-func.png)\n",
    "\n",
    "Digamma 函数:\n",
    "\n",
    "$\\Psi(x)=\\frac{d log\\Gamma(x)}{dx}$\n",
    "\n",
    "在涉及求 Dirichlet 分布相关的参数的极大似然估计时，往往需要使用到这个函数。\n",
    "\n",
    "### 1.3 从二项分布到 Gamma 分布\n",
    "$\\int_{0}^{\\infty}\\frac{x^{\\alpha-1}e^{-x}}{\\Gamma(\\alpha)}dx=1$\n",
    "\n",
    "取积分中的函数作为概率密度，就得到一个形式最简单的 Gamma 分布的密度函数\n",
    "\n",
    "$Gamma(x|\\alpha)=\\frac{x^{\\alpha-1}e^{-x}}{\\Gamma(\\alpha)}$\n",
    "\n",
    "$x=\\beta t$，就得到 Gamma 分布的更一般的形式\n",
    "\n",
    "$Gamma(x|\\alpha,\\beta)=\\frac{\\beta^{\\alpha}t^{\\alpha-1}e^{-\\beta t}}{\\Gamma(\\alpha)}$\n",
    "\n",
    "其中α称为 shape parameter，主要决定了分布曲线的形状; 而 β称为 rate parameter 或者 inverse scale parameter(1/β称为 scale parameter)，主要决定曲线有多陡。\n",
    "\n",
    "![5](https://uploads.cosx.org/2013/01/gamma-distribution.png)\n",
    "\n",
    "参数为 λ 的 Poisson 分布，概率写为\n",
    "\n",
    "$Possion(X=k|\\lambda)=\\frac{\\lambda^ke^{-\\lambda}}{k!}$\n",
    "\n",
    "在 Gamma 分布的密度中取 α=k+1 得到\n",
    "\n",
    "$Gamma(x|\\alpha=k+1)=\\frac{x^k e^{-x}}{\\Gamma(k+1)}=\\frac{x^k e^{-x}}{k!}$\n",
    "\n",
    "所以这两个分布数学形式上是一致的，只是 Poisson 分布是离散的，Gamma 分布是连续的，可以直观的认为 Gamma 分布是 Poisson 分布在正实数集上的连续化版本。 \n",
    "\n",
    "# 认识 Beta/Dirichlet 分布\n",
    "## 2. 认识 Beta/Dirichlet 分布\n",
    "### 2.1 魔鬼的游戏—认识 Beta 分布\n",
    "撒旦说：“你们人类很聪明，而我是很仁慈的，和你玩一个游戏，赢了就可以走，否则把灵魂出卖给我。游戏的规则很简单，我有一个魔盒，上面有一个按钮，你每按一下按钮，就均匀的输出一个 [0,1] 之间的随机数，我现在按 10 下，我手上有 10 个数，你猜第 7 大的数是什么，偏离不超过 0.01 就算对。”你应该怎么猜呢？\n",
    "\n",
    "上面这个游戏其实是在说随机变量 $X_1,X_2,\\cdots,X_n {\\stackrel{\\mathrm{iid}}{\\sim}} Uniform(0,1)$，把这 n 个随机变量排序后得到顺序统计量 $X_{(1)},X_{(2)},...,X_{(n)}$, 然后问 $X_{(k)}$ 的分布是什么。\n",
    "\n",
    "对于上面的游戏而言 n=10,k=7,如果我们能求出 X(7) 的分布的概率密度。\n",
    "\n",
    "假设 n 个数中只有一个落在了区间 [x,x+Δx] 内，则因为这个区间内的数 X(k) 是第 k 大的，则 [0,x) 中应该有 k−1 个数，(x,1] 这个区间中应该有 n−k 个数。不失一般性，我们先考虑如下一个符合上述要求的事件 E\n",
    "\n",
    "$E={X_1 \\in [x,x+\\Delta x],X_i \\in [0,x) (i=2,...,k),X_j \\in (x+\\Delta x,1] (j=k+1,...,n)}$\n",
    "\n",
    "![6](https://uploads.cosx.org/2013/01/beta-game-1.png)\n",
    "\n",
    "$P(E)=\\prod_{i=1}^n P(X_i)=x^{k-1}(1-x-\\Delta x)^{n-k}\\Delta x=x^{k-1}(1-x)^{n-k}\\Delta x + o(\\Delta x)$\n",
    "\n",
    "o(Δx) 表示 Δx 的高阶无穷小。显然，由于不同的排列组合，即 n 个数中有一个落在[x,x+Δx] 区间的有 n 种取法，余下 n−1 个数中有 k−1 个落在 [0,x) 的有 $\\binom{n-1}{k-1}$ 种组合，所以和事件 E 等价的事件一共有 $n\\binom{n-1}{k-1}$ 个。\n",
    "\n",
    "继续考虑稍微复杂一点情形，假设 n 个数中有两个数落在了区间 [x,x+Δx]，\n",
    "\n",
    "$E' = \\{ X_1,X_2\\in [x, x+\\Delta x], X_i \\in [0,x) \\quad (i=3,\\cdots,k), X_j \\in (x+\\Delta x,1] \\quad (j=k+1,\\cdots,n)\\}$\n",
    "\n",
    "![7](https://uploads.cosx.org/2013/01/beta-game-2.png)\n",
    "\n",
    "$P(E') = x^{k-2}(1-x-\\Delta x)^{n-k}(\\Delta x)^2 = o(\\Delta x)$\n",
    "\n",
    "从以上分析我们很容易看出，只要落在 [x,x+Δx] 内的数字超过一个，则对应的事件的概率就是 o(Δx)。于是\n",
    "\n",
    "$P( x \\le X_{(k)} \\le x+\\Delta x) = n\\binom{n-1}{k-1}P(E) + o(\\Delta x) = n\\binom{n-1}{k-1}x^{k-1}(1-x)^{n-k}\\Delta x + o(\\Delta x)$\n",
    "\n",
    "所以，可以得到 $X_{(k)}$ 的概率密度函数为\n",
    "\n",
    "$f(x) = \\lim_{\\Delta x \\rightarrow 0} \\frac{P( x \\le X_{(k)} \\le x+\\Delta x)}{\\Delta x} = n\\binom{n-1}{k-1}x^{k-1}(1-x)^{n-k} = \\frac{n!}{(k-1)!(n-k)!}x^{k-1}(1-x)^{n-k} \\quad x \\in [0,1]$\n",
    "\n",
    "利用 Gamma 函数，我们可以把 f(x) 表达为\n",
    "\n",
    "$f(x) = \\frac{\\Gamma(n+1)}{\\Gamma(k)\\Gamma(n-k+1)}x^{k-1}(1-x)^{n-k}$\n",
    "\n",
    "在上式中取 α=k,β=n−k+1, 于是我们得到\n",
    "\n",
    "$f(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1}$\n",
    "\n",
    "这个就是一般意义上的 Beta 分布！\n",
    "\n",
    "我们回到魔鬼的游戏，这 n=10,k=7 这个具体的实例中，我们按照如下密度分布的峰值去猜测才是最有把握的。\n",
    "\n",
    "$f(x) = \\frac{10!}{(6)!(3)!}x^{6}(1-x)^{3} \\quad x \\in [0,1]$\n",
    "\n",
    "然而即便如此，我们能做到一次猜中的概率也不高，很不幸，你第一次没有猜中，魔鬼微笑着说：“我再仁慈一点，再给你一个机会，你按 5 下这个机器，你就得到了 5 个 [0,1] 之间的随机数，然后我可以告诉你这 5 个数中的每一个和我的第 7 大的数相比，谁大谁小，然后你继续猜我手头的第 7 大的数是多少。”这时候我们应该怎么猜测呢？\n",
    "\n",
    "### 2.2 Beta-Binomial 共轭\n",
    "魔鬼的第二个题目，数学上形式化一下，就是\n",
    "\n",
    "1. $X_1,X_2,\\cdots,X_n {\\stackrel{\\mathrm{iid}}{\\sim}}Uniform(0,1)$，对应的顺序统计量是 $X_{(1)},X_{(2)},\\cdots, X_{(n)}$, 我们要猜测 $p=X_{(k)}$；\n",
    "1. $Y_1,Y_2,\\cdots,Y_m {\\stackrel{\\mathrm{iid}}{\\sim}}Uniform(0,1)$,$Y_i$中有\b m1 个比 p 小，m2 个比 p 大；\n",
    "1. 问 $P(p|Y_1,Y_2,\\cdots,Y_m)$ 的分布是什么。\n",
    "\n",
    "由于 p=X(k) 在 X1,X2,⋯,Xn 中是第 k 大的，利用 Yi 的信息，我们容易推理得到 p=X(k) 在 $X_1,X_2,\\cdots,X_n,Y_1,Y_2,\\cdots,Y_m {\\stackrel{\\mathrm{iid}}{\\sim}} Uniform(0,1)$ 这 (m+n) 个独立随机变量中是第 k+m1 大的，于是按照上一个小节的推理，此时 p=X(k) 的概率密度函数是 $Beta(p|k+m_1,n-k+1+m_2)$按照贝叶斯推理的逻辑，我们把以上过程整理如下：\n",
    "\n",
    "1. p=X(k) 是我们要猜测的参数，我们推导出 p 的分布为 $f(p) = Beta(p|k,n-k+1)$, 称为 p 的先验分布；\n",
    "1. 数据 Yi 中有 m1 个比 p 小，m2 个比 p 大，Yi 相当于是做了 m 次贝努利实验，所以 m1 服从二项分布 B(m,p)；\n",
    "1. 在给定了来自数据提供的 (m1,m2) 的知识后，p 的后验分布变为 $f(p|m_1,m_2)=Beta(p|k+m_1,n-k+1+m_2)$\n",
    "\n",
    "以上贝叶斯分析过程的简单直观的表述就是\n",
    "\n",
    "$Beta(p|k,n-k+1) + Count(m_1,m_2) = Beta(p|k+m_1,n-k+1+m_2)$\n",
    "\n",
    "其中 (m1,m2) 对应的是二项分布 B(m1+m2,p) 的计数。更一般的，对于非负实数 α,β，我们有如下关系\n",
    "\n",
    "$Beta(p|\\alpha,\\beta) + Count(m_1,m_2) = Beta(p|\\alpha+m_1,\\beta+m_2)$\n",
    "\n",
    "这个式子实际上描述的就是 Beta-Binomial 共轭，此处共轭的意思就是，数据符合二项分布的时候，参数的先验分布和后验分布都能保持 Beta 分布的形式，这种形式不变的好处是，我们能够在先验分布中赋予参数很明确的物理意义，这个物理意义可以延续到后验分布中进行解释，同时从先验变换到后验过程中从数据中补充的知识也容易有物理解释。\n",
    "\n",
    "$Beta(p|1,1) + Count(\\alpha-1,\\beta-1) = Beta(p|\\alpha,\\beta)  \\quad  (***)$\n",
    "\n",
    "其中 Beta(p|1,1) 恰好就是均匀分布 Uniform(0,1)。\n",
    "\n",
    "对于 (***) 式，我们其实也可以纯粹从贝叶斯的角度来进行推导和理解。 假设有一个不均匀的硬币抛出正面的概率为 p, 抛 m 次后出现正面和反面的次数分别是 m1,m2，那么按传统的频率学派观点，p的估计值应该为 $\\hat{p}=\\frac{m_1}{m}$。而从贝叶斯学派的观点来看，开始对硬币不均匀性一无所知，所以应该假设 p∼Uniform(0,1), 于是有了二项分布的计数 (m1,m2)之后，按照贝叶斯公式如下计算 p 的后验分布\n",
    "\n",
    "$P(p|m_1,m_2) = \\frac{P(p)\\cdot P(m_1,m_2|p)}{P(m_1,m_2)} = \\frac{1\\cdot P(m_1,m_2|p)}{\\int_0^1 P(m_1,m_2|t)dt} = \\frac{\\binom{m}{m_1}p^{m_1}(1-p)^{m_2}}{\\int_0^1 \\binom{m}{m_1}t^{m_1}(1-t)^{m_2}dt} = \\frac{p^{m_1}(1-p)^{m_2}}{\\int_0^1 t^{m_1}(1-t)^{m_2}dt}$\n",
    "\n",
    "计算得到的后验分布正好是 $Beta(p|m_1+1,m_2+1)$。\n",
    "\n",
    "最后我们再回到魔鬼的游戏，如果你按出的 5 个随机数字中，魔鬼告诉你有 2 个小于它手中第 7 大的数，那么你应该\n",
    "\n",
    "$Beta(x|9,7) = \\frac{15!}{(8)!(6)!}x^{8}(1-x)^{6} \\quad x \\in [0,1]$\n",
    "\n",
    "很幸运的，你这次猜中了，魔鬼开始甩赖了：这个游戏对你来说太简单了，我要加大点难度，我们重新来一次，我按魔盒 20 下生成 20 个随机数，你同时给我猜第 7 大和第 13 大的数是什么，这时候应该如何猜测呢？\n",
    "\n",
    "### 2.3 Dirichlet-Multinomial 共轭\n",
    "对于魔鬼变本加厉的新的游戏规则，数学形式化如下：\n",
    "\n",
    "1. $X_1,X_2,\\cdots,X_n {\\stackrel{\\mathrm{iid}} {\\sim}}Uniform(0,1)$,\n",
    "1. 排序后对应的顺序统计量 $X_{(1)},X_{(2)},\\cdots, X_{(n)}$,\n",
    "1. 问 $(X_{(k_1)}, X_{(k_1+k_2)})$ 的联合分布是什么；\n",
    "\n",
    "完全类似于第一个游戏的推导过程，我们可以进行如下的概率计算 (为了数学公式的简洁对称，我们取 x3 满足 x1+x2+x3=1, 但只有 x1,x2是变量)\n",
    "\n",
    "![8](https://uploads.cosx.org/2013/01/dirichlet-game.png)\n",
    "\n",
    "$(X_{(k_1)}, X_{(k_1+k_2)})$ 的联合分布推导\n",
    "\n",
    "$P\\Bigl(X_{(k_1)}\\in(x_1,x_1+\\Delta x),X_{(k_1+k_2)}\\in(x_2,x_2+\\Delta x)\\Bigr) = n(n-1)\\binom{n-2}{k_1-1,k_2-1}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\\Delta x)^2 = \\frac{n!}{(k_1-1)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}(\\Delta x)^2$\n",
    "\n",
    "于是我们得到 $(X_{(k_1)}, X_{(k_1+k_2)})$ 的联合分布是\n",
    "\n",
    "$f(x_1,x_2,x_3) = \\frac{n!}{(k_1-1)!(k_2-1)!(n-k_1-k_2)!}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2} = \\frac{\\Gamma(n+1)}{\\Gamma(k_1)\\Gamma(k_2)\\Gamma(n-k_1-k_2+1)}x_1^{k_1-1}x_2^{k_2-1}x_3^{n-k_1-k_2}$\n",
    "\n",
    "上面这个分布其实就是 3 维形式的 Dirichlet 分布 $Dir(x_1,x_2,x_3|k_1,k_2,n-k_1-k_2+1)$。令 $\\alpha_1=k_1,\\alpha_2=k_2,\\alpha_3=n-k_1-k_2+1$, 于是分布密度可以写为\n",
    "\n",
    "$f(x_1,x_2,x_3) = \\frac{\\Gamma(\\alpha_1 + \\alpha_2 + \\alpha_3)}{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)\\Gamma(\\alpha_3)}x_1^{\\alpha_1-1}x_2^{\\alpha_2-1}x_3^{\\alpha_3-1}$\n",
    "\n",
    "从形式上我们也能看出，Dirichlet 分布是 Beta 分布在高维度上的推广。\n",
    "\n",
    "类似于魔鬼的游戏 2，我们也可以调整一下游戏 3，从魔盒中生成 m 个随机数 \n",
    "\n",
    "$Y_1,Y_2,\\cdots,Y_m {\\stackrel{\\mathrm{iid}}{\\sim}}Uniform(0,1)$ 并让魔鬼告诉我们 $Y_i$ 和 $(X_{(k_1)}, X_{(k_1+k_2)})$ 相比谁大谁小。于是有如下游戏 4\n",
    "\n",
    "1. $X_1,X_2,\\cdots,X_n {\\stackrel{\\mathrm{iid}}{\\sim}}Uniform(0,1)$，排序后对应的顺序统计量 $X_{(1)},X_{(2)},\\cdots, X_{(n)}$\n",
    "1. 令$p_1=X_{(k_1)}, p_2=X_{(k_1+k_2)},p_3 = 1-p_1-p_2$(加上 p3 是为了数学表达简洁对称), 我们要猜测 $\\vec{p}=(p_1,p_2,p_3)$\n",
    "1. $Y_1,Y_2,\\cdots,Y_m {\\stackrel{\\mathrm{iid}}{\\sim}}Uniform(0,1)$, Yi 中落到 [0,p1),[p1,p2),[p2,1] 三个区间的个数分别为 m1,m2,m3，m=m1+m2+m3；\n",
    "1. 问后验分布 $P(\\vec{p}|Y_1,Y_2,\\cdots,Y_m)$ 的分布是什么。\n",
    "\n",
    "为了方便，我们记\n",
    "\n",
    "$\\vec{m}=(m_1,m_2,m_3),\\quad \\vec{k}=(k_1,k_2,n-k_1-k_2+1)$\n",
    "\n",
    "由游戏中的信息，我们可以推理得到 p1,p2 在 $X_1,X_2,\\cdots,X_n,Y_1,Y_2,\\cdots,Y_m{\\stackrel{\\mathrm{iid}}{\\sim}} Uniform(0,1)$这 m+n 个数中分别成为了第 k1+m1,k2+m2 大的数，于是后验分布 $P(\\vec{p}|Y_1,Y_2,\\cdots,Y_m)$ 应该是 $Dir(\\vec{p}|k_1+m_1,k_1+m_2,n-k_1-k_2+1+m_3)$即$Dir(\\vec{p}|\\vec{k}+\\vec{m})$。按照贝叶斯推理的逻辑，我们同样可以把以上过程整理如下：\n",
    "\n",
    "1. 我们要猜测参数 $\\vec{p}=(p_1,p_2,p_3)$，其先验分布为 $Dir(\\vec{p}|\\vec{k})$\n",
    "1. 数据 Yi 落到 [0,p1),[p1,p2),[p2,1] 三个区间的个数分别为 m1,m2,m3，所以 $\\vec{m}=(m_1,m_2,m_3)$ 服从多项分布 $Mult(\\vec{m}|\\vec{p})$\n",
    "1. 在给定了来自数据提供的知识 $\\vec{m}$ 后，$\\vec{p}$ 的后验分布变为 $Dir(\\vec{p}|\\vec{k}+\\vec{m})$\n",
    "\n",
    "以上贝叶斯分析过程的简单直观的表述就是\n",
    "\n",
    "$Dir(\\vec{p}|\\vec{k}) + MultCount(\\vec{m}) = Dir(\\vec{p}|\\vec{k}+\\vec{m})$\n",
    "\n",
    "令 $\\vec{\\alpha}=\\vec{k}$, 把 $\\vec{\\alpha}$ 从整数集合延拓到实数集合，更一般的可以证明有如下关系\n",
    "\n",
    "$Dir(\\vec{p}|\\vec{\\alpha}) + MultCount(\\vec{m}) = Dir(p|\\vec{\\alpha}+\\vec{m})$\n",
    "\n",
    "以上式子实际上描述的就是 Dirichlet-Multinomial 共轭，而我们从以上过程可以看到，Dirichlet 分布中的参数 $\\vec{\\alpha}$  都可以理解为物理计数。类似于 Beta 分布，我们也可以把 $Dir(\\vec{p}|\\vec{\\alpha})$ 作如下分解\n",
    "\n",
    "$Dir(\\vec{p}|\\vec{1}) + MultCount(\\vec{m}-\\vec{1})= Dir(\\vec{p}|\\vec{\\alpha})$\n",
    "\n",
    "此处 $\\vec{1}=(1,1,\\cdots,1)$。自然，上式我们也可以类似地用纯粹贝叶斯的观点进行推导和解释。\n",
    "\n",
    "以上的游戏我们还可以往更高的维度上继续推，譬如猜测 $X_{(1)},X_{(2)},\\cdots, X_{(n)}$ 中的 4、5、… 等更多个数，于是就得到更高纬度的 Dirichlet 分布和 Dirichlet-Multinomial 共轭。一般形式的 Dirichlet 分布定义如下\n",
    "\n",
    "$Dir(\\vec{p}|\\vec{\\alpha}) =\\frac{\\Gamma(\\sum_{k=1}^K\\alpha_k)}{\\prod_{k=1}^K\\Gamma(\\alpha_k)} \\prod_{k=1}^K p_k^{\\alpha_k-1}$\n",
    "\n",
    "对于给定的 $\\vec{p}$和 N, 多项分布定义为\n",
    "\n",
    "$Mult(\\vec{n} |\\vec{p},N) = \\binom{N}{\\vec{n}}\\prod_{k=1}^K p_k^{n_k}$\n",
    "\n",
    "而 $Mult(\\vec{n} |\\vec{p},N)$ 和 $Dir(\\vec{p}|\\vec{\\alpha})$ 这两个分布是共轭关系。\n",
    "\n",
    "### 2.4 Beta/Dirichlet 分布的一个性质\n",
    "如果 $p\\sim Beta(t|\\alpha,\\beta)$, 则\n",
    "\n",
    "$E(p) = \\int_0^1 t*Beta(t|\\alpha,\\beta)dt = \\int_0^1 t* \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} t^{\\alpha-1}(1-t)^{\\beta-1}dt = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^1 t^{\\alpha}(1-t)^{\\beta-1}dt$\n",
    "\n",
    "上式右边的积分对应到概率分布 $Beta(t|\\alpha+1,\\beta)$，对于这个分布，我们有\n",
    "\n",
    "$\\int_0^1 \\frac{\\Gamma(\\alpha+\\beta+1)}{\\Gamma(\\alpha+1)\\Gamma(\\beta)} t^{\\alpha}(1-t)^{\\beta-1}dt = 1$\n",
    "\n",
    "把上式带入 E(p) 的计算式，得到\n",
    "\n",
    "$E(p) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\cdot \\frac{\\Gamma(\\alpha+1)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta+1)}= \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha+\\beta+1)}\\frac{\\Gamma(\\alpha+1)}{\\Gamma(\\alpha)} = \\frac{\\alpha}{\\alpha+\\beta}$\n",
    "\n",
    "这说明，对于 Beta 分布的随机变量，其均值可以用 $\\frac{\\alpha}{\\alpha+\\beta}$ 来估计。Dirichlet 分布也有类似的结论，如果 $\\vec{p} \\sim Dir(\\vec{t}|\\vec{\\alpha})$，同样可以证明\n",
    "\n",
    "$E(\\vec{p}) = \\Bigl(\\frac{\\alpha_1}{\\sum_{i=1}^K\\alpha_i},\\frac{\\alpha_2}{\\sum_{i=1}^K\\alpha_i},\\cdots, \\frac{\\alpha_K}{\\sum_{i=1}^K\\alpha_i} \\Bigr)$\n",
    "\n",
    "# MCMC 和 Gibbs Sampling\n",
    "## MCMC 和 Gibbs Sampling\n",
    "### 3.1 随机模拟\n",
    "#### 蒙特卡罗方法\n",
    "统计模拟中有一个重要的问题就是给定一个概率分布 p(x)，我们如何在计算机中生成它的样本。一般而言均匀分布 Uniform(0,1) 的样本是相对容易生成的。 通过线性同余发生器可以生成伪随机数，我们用确定性算法生成 [0,1] 之间的伪随机数序列后，这些序列的各种统计指标和均匀分布 Uniform(0,1) 的理论计算结果非常接近。这样的伪随机序列就有比较好的统计性质，可以被当成真实的随机数使用。\n",
    "\n",
    "#### 生成一个概率分布的样本\n",
    "而我们常见的概率分布，无论是连续的还是离散的分布，都可以基于 Uniform(0,1) 的样本生成。例如正态分布可以通过著名的 Box-Muller 变换得到\n",
    "\n",
    "[Box-Muller 变换] 如果随机变量 U1,U2 独立且$U_1, U_2 \\sim Uniform[0,1]$\n",
    "\n",
    "$Z_0 = \\sqrt{-2\\ln U_1} cos(2\\pi U_2) Z_1 = \\sqrt{-2\\ln U_1} sin(2\\pi U_2)$\n",
    "\n",
    "则 Z0,Z1 独立且服从标准正态分布。\n",
    "\n",
    "不过我们并不是总是这么幸运的，当 p(x) 的形式很复杂，或者 p(x) 是个高维的分布的时候，样本的生成就可能很困难了。\n",
    "\n",
    "### 3.2 马氏链及其平稳分布\n",
    "马氏链的数学定义\n",
    "\n",
    "$P(X_{t+1}=x|X_t, X_{t-1}, \\cdots) =P(X_{t+1}=x|X_t)$\n",
    "\n",
    "也就是状态转移的概率只依赖于前一个状态。\n",
    "\n",
    "从父代到子代，收入阶层的变化的转移概率如下\n",
    "\n",
    "![9](https://uploads.cosx.org/2013/01/table-1.jpg)\n",
    "\n",
    "![10](https://uploads.cosx.org/2013/01/markov-transition.png)\n",
    "\n",
    "使用矩阵的表示方式，转移概率矩阵记为\n",
    "\n",
    "math\n",
    "P =\n",
    "\\begin{bmatrix}\n",
    "0.65 & 0.28 & 0.07 \\\\\n",
    "0.15 & 0.67 & 0.18 \\\\\n",
    "0.12 & 0.36 & 0.52 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "假设当前这一代人处在下层、中层、上层的人的比例是概率分布向量 π0=[π0(1),π0(2),π0(3)]，那么他们的子女的分布比例将是 $\\pi_1=\\pi_0 P$, 他们的孙子代的分布比例将是 $\\pi_2=\\pi_1 p=\\pi_0 P^2$, ……, 第 n 代子孙的收入分布比例将是$\\pi_n=\\pi_{n-1}=\\pi_0 P^n$。\n",
    "\n",
    "假设初始概率分布为 π0=[0.21,0.68,0.11]，则我们可以计算前 n 代人的分布状况如下\n",
    "\n",
    "![11](https://uploads.cosx.org/2013/01/table-2.jpg)\n",
    "\n",
    "我们发现从第 7 代人开始，这个分布就稳定不变了。我们换一个初始概率分布 π0=[0.75,0.15,0.1] 试试看，继续计算前 n 代人的分布状况如下\n",
    "\n",
    "![12](https://uploads.cosx.org/2013/01/table-3.jpg)\n",
    "\n",
    "我们发现，到第 9 代人的时候, 分布又收敛了。最为奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布 π=[0.286,0.489,0.225]，也就是说收敛的行为和初始概率分布 π0 无关。这说明这个收敛行为主要是由概率转移矩阵 $P$ 决定的。我们计算一下 $P^n$\n",
    "\n",
    "math\n",
    "P^{20} = P^{21} = \\cdots = P^{100} = \\cdots =\n",
    "\\begin{bmatrix}\n",
    "0.286 & 0.489 & 0.225 \\\\\n",
    "0.286 & 0.489 & 0.225 \\\\\n",
    "0.286 & 0.489 & 0.225 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "马氏链定理： 如果一个非周期马氏链具有转移概率矩阵 P, 且它的任何两个状态是连通的，那么 $\\displaystyle \\lim_{n\\rightarrow\\infty}P_{ij}^n$存在且与 \n",
    "i 无关，记 $\\displaystyle \\lim_{n\\rightarrow\\infty}P_{ij}^n = \\pi(j)$， 我们有\n",
    "\n",
    "1 .\n",
    "\n",
    "math\n",
    "\\displaystyle \\lim_{n \\rightarrow \\infty} P^n =\\begin{bmatrix}\n",
    "\\pi(1) & \\pi(2) & \\cdots & \\pi(j) & \\cdots \\\\\n",
    "\\pi(1) & \\pi(2) & \\cdots & \\pi(j) & \\cdots \\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots & \\cdots \\\\  \n",
    "\\pi(1) & \\pi(2) & \\cdots & \\pi(j) & \\cdots \\\\ \n",
    "\\cdots & \\cdots & \\cdots & \\cdots & \\cdots \\\\ \n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "2 . $\\displaystyle \\pi(j) = \\sum_{i=0}^{\\infty}\\pi(i)P_{ij}$\n",
    "\n",
    "3 . π是方程 πP=π 的唯一非负解。其中,$\\pi = [\\pi(1), \\pi(2), \\cdots, \\pi(j),\\cdots ], \\quad \\sum_{i=0}^{\\infty} \\pi_i = 1$\n",
    "\n",
    "π 称为马氏链的平稳分布。\n",
    "\n",
    "这个马氏链的收敛定理非常重要，所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的。\n",
    "\n",
    "### 3.3 Markov Chain Monte Carlo\n",
    "对于给定的概率分布 p(x), 我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布， 于是一个很的漂亮想法是：如果我们能构造一个转移矩阵为 $P$ 的马氏链，使得该马氏链的平稳分布恰好是 p(x)， 那么我们从任何一个初始状态 x0 出发沿着马氏链转移, 得到一个转移序列 0,x1,x2,⋯xn,xn+1⋯,， 如果马氏链在第 n 步已经收敛了，于是我们就得到了 π(x) 的样本 xn,xn+1⋯。\n",
    "\n",
    "定理：[细致平稳条件] 如果非周期马氏链的转移矩阵 P 和分布 π(x) 满足\n",
    "\n",
    "$\\pi(i)P_{ij} = \\pi(j)P_{ji} \\quad\\quad \\text{for all} \\quad i,j$\n",
    "\n",
    "则 π(x) 是马氏链的平稳分布，上式被称为细致平稳条件 (detailed balance condition)。由细致平稳条件可得\n",
    "\n",
    "$\\sum_{i=1}^\\infty \\pi(i)P_{ij} = \\sum_{i=1}^\\infty \\pi(j)P_{ji} = \\pi(j) \\sum_{i=1}^\\infty P_{ji} = \\pi(j) \\Rightarrow \\pi P = \\pi$\n",
    "\n",
    "由于 π 是方程 πP=π 的解，所以 π 是平稳分布。\n",
    "\n",
    "假设我们已经有一个转移矩阵为 Q马氏链 (q(i,j) 表示从状态 i转移到状态 j的概率，也可以写为 q(j|i)或者 q(i→j)), 显然，通常情况下\n",
    "\n",
    "$p(i) q(i,j) \\neq p(j) q(j,i)$\n",
    "\n",
    "也就是细致平稳条件不成立，所以 p(x)不太可能是这个马氏链的平稳分布。我们可否对马氏链做一个改造，使得细致平稳条件成立呢？譬如，我们引入一个 α(i,j)，我们希望\n",
    "\n",
    "$p(i) q(i,j)\\alpha(i,j) = p(j) q(j,i)\\alpha(j,i)  \\quad (*)$\n",
    "\n",
    "按照对称性，我们可以取\n",
    "\n",
    "$\\alpha(i,j)= p(j) q(j,i), \\quad \\alpha(j,i) = p(i) q(i,j)$\n",
    "\n",
    "于是 (*) 式就成立了。所以有\n",
    "\n",
    "$p(i) {q(i,j)\\alpha(i,j)}_{Q'(i,j)}= p(j) {q(j,i)\\alpha(j,i)}_{Q'(j,i)}  \\quad (**)$\n",
    "\n",
    "在改造 Q 的过程中引入的 α(i,j) 称为接受率，物理意义可以理解为在原来的马氏链上，从状态 i 以 q(i,j) 的概率转跳转到状态 j 的时候，我们以 α(i,j) 的概率接受这个转移，于是得到新的马氏链 Q′ 的转移概率为 q(i,j)α(i,j)。\n",
    "\n",
    "![11](https://uploads.cosx.org/2013/01/mcmc-transition.jpg)\n",
    "\n",
    "![12](https://uploads.cosx.org/2013/01/mcmc-algo-1.jpg)\n",
    "\n",
    "假设 α(i,j)=0.1,α(j,i)=0.2, 此时满足细致平稳条件，于是 $p(i)q(i,j)\\times 0.1 = p(j)q(j,i) \\times 0.2$\n",
    "\n",
    "上式两边扩大 5 倍，我们改写为 $p(i)q(i,j) \\times 0.5 = p(j)q(j,i) \\times 1$\n",
    "\n",
    "看，我们提高了接受率，而细致平稳条件并没有打破！这启发我们可以把细致平稳条件 (**) 式中的 α(i,j),α(j,i) 同比例放大，使得两数中最大的一个放大到 1，这样我们就提高了采样中的跳转接受率。所以我们可以取\n",
    "\n",
    "$\\alpha(i,j) = \\min\\left\\{\\frac{p(j)q(j,i)}{p(i)q(i,j)},1\\right\\}$\n",
    "\n",
    "于是，经过对上述 MCMC 采样算法中接受率的微小改造，我们就得到了如下教科书中最常见的 Metropolis-Hastings 算法。\n",
    "\n",
    "![13](https://uploads.cosx.org/2013/01/mcmc-algo-2.jpg)\n",
    "\n",
    "对于分布 p(x), 我们构造转移矩阵 Q′ 使其满足细致平稳条件\n",
    "\n",
    "$p(x) Q'(x\\rightarrow y) = p(y) Q'(y\\rightarrow x)$\n",
    "\n",
    "此处 x 并不要求是一维的，对于高维空间的 p(x)，如果满足细致平稳条件\n",
    "\n",
    "$p(\\mathbf{x}) Q'(\\mathbf{x}\\rightarrow \\mathbf{y}) = p(\\mathbf{y}) Q'(\\mathbf{y}\\rightarrow \\mathbf{x})$\n",
    "\n",
    "那么以上的 Metropolis-Hastings 算法一样有效。\n",
    "\n",
    "### 3.4 Gibbs Sampling\n",
    "对于高维的情形，由于接受率 α 的存在 (通常 α<1)，以上 Metropolis-Hastings 算法的效率不够高。能否找到一个转移矩阵 Q 使得接受率 α=1 呢？\n",
    "\n",
    "![14](https://uploads.cosx.org/2013/01/gibbs-transition.png)\n",
    "\n",
    "我们可以如下构造平面上任意两点之间的转移概率矩阵 Q\n",
    "\n",
    "$Q(A\\rightarrow B) = p(y_B|x_1) \\text{if} \\quad x_A=x_B=x_1$\n",
    "\n",
    "$Q(A\\rightarrow C) = p(x_C|y_1) \\text{if} \\quad y_A=y_C=y_1$\n",
    "\n",
    "$Q(A\\rightarrow D) = 0 \\text{other}$\n",
    "\n",
    "有了如上的转移矩阵 Q，我们很容易验证对平面上任意两点 X,Y,满足细致平稳条件\n",
    "\n",
    "$p(X)Q(X\\rightarrow Y) = p(Y) Q(Y\\rightarrow X)$\n",
    "\n",
    "于是这个二维空间上的马氏链将收敛到平稳分布 p(x,y)。而这个算法就称为 Gibbs Sampling 算法。\n",
    "\n",
    "![15](https://uploads.cosx.org/2013/01/gibbs-algo-1.jpg)\n",
    "\n",
    "# 参考资料\n",
    "- [LDA-math - 神奇的 Gamma 函数](https://cosx.org/2013/01/lda-math-gamma-function/)\n",
    "- [LDA-math - 认识 Beta/Dirichlet 分布](https://cosx.org/2013/01/lda-math-beta-dirichlet/)\n",
    "- [LDA-math-MCMC 和 Gibbs Sampling](https://cosx.org/2013/01/lda-math-mcmc-and-gibbs-sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
