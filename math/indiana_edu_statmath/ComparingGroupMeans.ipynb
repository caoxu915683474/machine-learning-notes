{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Comparing Group Means: T-tests and One-way ANOVA\n",
    "https://scholarworks.iu.edu/dspace/handle/2022/19735\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "### 1.1 Background of the T-test: Key Assumptions\n",
    "The t-test assumes that **samples are randomly drawn from normally distributed populations with unknown population variances.** If such assumption cannot be made, you may try nonparametric methods. The variables of interest should be **random variables**, whose values change randomly. A constant such as the number of parents of a person is not a random variable. In addition, the occurrence of one measurement in a variable should be independent of the occurrence of others. In other word, the occurrence of an event does not change the probability that other events occur. This property is called **statistical independence**. Time series data are likely to be statistically dependent because they are often autocorrelated.\n",
    "\n",
    "T-tests assume **random sampling without any selection bias**. If a researcher intentionally selects some samples with properties that he prefers and then compares them with other samples, his inferences based on this non-random sampling are neither reliable nor generalized. In an experiment, a subject should be randomly assigned to either the control or treated group so that two groups do not have any systematic difference except for the treatment applied. When subjects can decide whether or not to participate (non-random assignment), however, the independent sample t-test may under- or over-estimate the difference between the control and treated groups. In this case of self-selection, the propensity score matching and treatment effect model may produce robust and reliable estimates of mean differences.\n",
    "\n",
    "Another, yet closely related to random sampling, key component is **population normality**. If this assumption is violated, a sample mean is no longer the best measure (unbiased estimator) of central tendency and t-test will not be valid. Figure 1 illustrates the standard normal probability distribution on the left and a bimodal distribution on the right. Even if the two distributions have the same mean and variance, we cannot say much about their mean difference.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//17-11-19/957879.jpg)\n",
    "\n",
    "The violation of normality becomes more problematic in the one-tailed test than the two-tailed one (Hildebrand et al. 2005: 329). Figure 2 shows how the violation influences statistical inferences. The left red curve indicates the standard normal probability distribution with its 1 percent one-tailed rejection region on the left. The blue one is for a non-normal distribution with the blue 1 percent rejection region (critical region). The test statistic indicated by a vertical green line falls in the rejection region of the skewed non-normal distribution but does not in the red shaded area of the standard normal distribution. If the populations follow such a non-normal distribution, the one-tailed t-test based on the normality does not mistakenly reject the null hypothesis.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//17-11-19/65932100.jpg)\n",
    "\n",
    "**Due to the Central Limit Theorem, the normality assumption is not as problematic as imagined in the real world.** The Theorem says that the distribution of a sample mean (e.g., $\\bar{y_1}$ and $\\bar{y_2}$ ) is\n",
    "approximately normal when its sample size is sufficiently large. **When $n_1 + n_2 \\ge 30$ , in practice, you do not need to worry too much about normality.**\n",
    "\n",
    "When sample size is small and normality is questionable, you might draw a histogram, P-P plot, and Q-Q plots or conduct the Shapiro-Wilk W (N<=2000), Shapiro-Francia W (N<=5000), Kolmogorov-Smirnov D (N>2000), and Jarque-Bera tests. If the normality assumption is violated, you might try such nonparametric methods as the Kolmogorov-Smirnov Test, Kruscal-Wallis Test, or Wilcoxon Rank-Sum Test.\n",
    "\n",
    "### 1.2 T-test and Analysis of Variance\n",
    "The t-test can be conducted on a one sample, paired samples, and independent samples. **The one sample t-test checks if the population mean is different from a hypothesized value (oftentimes zero).** If you have two samples, which are not independent but paired, you need to compute differences of individual matched pairs. A typical example is outcome measurements of pre- and post- treatment. **The paired t-test examines if the mean of the differences (effect of treatment) is discernable from zero (no effect). Therefore, the underlying methods of one-sample t-test and paired t-test are in fact identical.**\n",
    "\n",
    "If two samples are taken from different populations and their elements are not paired, the independent sample t-test compares the means of two samples. In a GPA data set of male and female students, for example, the GPA of the first male student is nothing to do with that of the first female student. When two samples have the same population variance, the independent samples t-test uses the pooled variance when computing standard error. Otherwise, individual variances need to be used instead in computation, and degrees of freedom should be approximated. The folded F test is used to evaluate the equality of two variances. In both cases, the null hypothesis is two samples have the same mean. Figure 3 illustrates these four types of t-tests and one way ANOVA.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//17-11-19/46607077.jpg)\n",
    "\n",
    "While the independent sample t-test is limited to comparing the means of two groups, the one-way ANOVA (Analysis of Variance) can compare more than two groups. ANOVA use F statistic to test if all groups have the same mean. Therefore, the t-test is considered a special case of the one-way ANOVA. When comparing means of two groups (one degree of freedom), the t statistic is the square root of the F statistic of ANOVA ($F=t^2$). But DO NOT be confused with the folded F test for examining the equality of the two variances.\n",
    "\n",
    "These analyses do not necessarily posit any causal relationship between the left-hand and right- hand side variables. Whether data are balanced(the numbers of observations across groups are not necessarily equal) does not matter in the t-test and one-way ANOVA. Table 1 compares the independent sample t-test and one-way ANOVA.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//17-11-19/26292397.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
