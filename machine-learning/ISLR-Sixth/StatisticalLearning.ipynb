{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TOC -->\n",
    "\n",
    "- [2.Statistical Learning](#2statistical-learning)\n",
    "    - [2.1 What Is Statistical Learning?](#21-what-is-statistical-learning)\n",
    "        - [2.1.1 Why Estimate f?](#211-why-estimate-f)\n",
    "            - [Prediction](#prediction)\n",
    "            - [Inference](#inference)\n",
    "        - [2.1.2 How Do We Estimate f?](#212-how-do-we-estimate-f)\n",
    "        - [2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability](#213-the-trade-off-between-prediction-accuracy-and-model-interpretability)\n",
    "        - [2.1.4 Supervised Versus Unsupervised Learning](#214-supervised-versus-unsupervised-learning)\n",
    "        - [2.1.5 Regression Versus Classification Problems](#215-regression-versus-classification-problems)\n",
    "    - [2.2 Assessing Model Accuracy](#22-assessing-model-accuracy)\n",
    "        - [2.2.1 Measuring the Quality of Fit](#221-measuring-the-quality-of-fit)\n",
    "        - [2.2.2 The Bias-Variance Trade-Off](#222-the-bias-variance-trade-off)\n",
    "        - [2.2.3 The Classification Setting](#223-the-classification-setting)\n",
    "\n",
    "<!-- /TOC -->\n",
    "\n",
    "# 2.Statistical Learning\n",
    "## 2.1 What Is Statistical Learning?\n",
    "In this setting, the advertising budgets are `input variables` while sales is an `output variable`. The input variables are typically denoted using the symbol X, with a subscript to distinguish them. So $X_1$ might be the TV budget, $X_2$ the radio budget, and $X_3$ the newspaper budget. The inputs go by different names, such as `predictors`, `independent variables`, `features`, or sometimes just `variables`. The output variable—in this case, sales—is often called the `response` or `dependent variable`, and is typically denoted using the symbol Y .\n",
    "\n",
    "![2.1](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/69750700.jpg)\n",
    "\n",
    "More generally, suppose that we observe a quantitative response Y and p different predictors, $X_1, X_2, \\cdots , X_p$. We assume that there is some relationship between Y and $X = (X_1,X_2,\\cdots,X_p)$, which can be written in the very general form\n",
    "\n",
    "$Y=f(X)+\\epsilon\\ (2.1)$\n",
    "\n",
    "Here f is some fixed but unknown function of $X_1, \\cdots , X_p$, and ε is a random `error term`, which is independent of X and has mean zero. In this formulation, f represents the `systematic` information that X provides about Y .\n",
    "\n",
    "![2.2](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/79722643.jpg)\n",
    "\n",
    "![2.3](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/56693792.jpg)\n",
    "\n",
    "**In essence, statistical learning refers to a set of approaches for estimating f .**\n",
    "\n",
    "### 2.1.1 Why Estimate f?\n",
    "There are two main reasons that we may wish to estimate f: `prediction` and `inference`.\n",
    "\n",
    "#### Prediction\n",
    "We can predict Y using\n",
    "\n",
    "$\\hat{Y}=\\hat{f}(X)\\ (2.2)$\n",
    "\n",
    "- $\\hat{f}$ represents our estimate for f\n",
    "- $\\hat{Y}$ represents the resulting prediction for Y\n",
    "\n",
    "$E(Y-\\hat{Y})^2=E[f(X)+\\epsilon-\\hat{f}(X)]^2=[f(X)-\\hat{f}(X)]^2+Var(\\epsilon)\\ (2.3)$\n",
    "\n",
    "- $E(Y-\\hat{Y})^2$ represents the average, or expected value, of the squared difference between the predicted and actual value of Y\n",
    "- `reducible error`:$[f(X)-\\hat{f}(X)]^2$\n",
    "- `irreducible error`:$Var(\\epsilon)$\n",
    "\n",
    "**The focus of this book is on techniques for estimating f with the aim of minimizing the reducible error.** It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for Y . This bound is almost always unknown in practice.\n",
    "\n",
    "#### Inference\n",
    "- Which predictors are associated with the response? \n",
    "- What is the relationship between the response and each predictor?\n",
    "- Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? \n",
    "\n",
    "### 2.1.2 How Do We Estimate f?\n",
    "Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function f. In other words, we want to find a function f such that $Y \\approx \\hat{f}(X)$ for any observation (X, Y).\n",
    "\n",
    "`Parametric methods` involve a two-step model-based approach.\n",
    "\n",
    "1. First, we make an assumption about the functional form, or shape, of f. For example, one very simple assumption is that f is linear in X:$f(X)=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\cdots+\\beta_pX_p\\ (2.4)$\n",
    "    - Once we have assumed that f is linear, instead of having to estimate an entirely arbitrary p-dimensional function f(X), one only needs to estimate the p+1 coefficients β0,β1,...,βp.\n",
    "1. After a model has been selected, we need a procedure that uses the `training data` to `fit` or `train` the model.\n",
    "\n",
    "If the chosen model is too far from the true f, then our estimate will be poor. We can try to address this problem by choosing `flexible` models that can fit many different possible functional forms for f. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as `overfitting` the data, which essentially means they follow the errors, or `noise`, too closely.\n",
    "\n",
    "`Non-parametric methods` do not make explicit assumptions about the func- tional form of f . Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. \n",
    "\n",
    "### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability\n",
    "![2.7](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/19837514.jpg)\n",
    "\n",
    "### 2.1.4 Supervised Versus Unsupervised Learning\n",
    "Many classical statistical learning methods such as linear regression and logistic regression (Chapter 4), as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.\n",
    "\n",
    "In contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation i = 1,...,n, we observe a vector of measurements xi but no associated response yi.\n",
    "\n",
    "### 2.1.5 Regression Versus Classification Problems\n",
    "Variables can be characterized as either `quantitative` or `qualitative` (also known as `categorical`). Quantitative variables take on numerical values. In contrast, qualitative variables take on values in one of K different `classes`, or categories.\n",
    "\n",
    "## 2.2 Assessing Model Accuracy\n",
    "### 2.2.1 Measuring the Quality of Fit\n",
    "In the regression setting, the most commonly-used measure is the `mean squared error (MSE)`, given by\n",
    "\n",
    "$MSE=\\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2\\ (2.5)$\n",
    "\n",
    "- $\\hat{f}(x_i)$ is the prediction that $\\hat{f}$ gives for the ith observation.\n",
    "\n",
    "![2.9](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/1980574.jpg)\n",
    "\n",
    "We now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function of flexibility, or more formally the `degrees of freedom`, for a number of smoothing splines.\n",
    "\n",
    "In the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a `U-shape` in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be `overfitting` the data.\n",
    "\n",
    "![2.10](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/13381650.jpg)\n",
    "\n",
    "Figure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a U-shape in the test MSE. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve.\n",
    "\n",
    "![2.11](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/15955420.jpg)\n",
    "\n",
    "Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly.\n",
    "\n",
    "### 2.2.2 The Bias-Variance Trade-Off\n",
    "The expected test MSE, for a given value $x_0$, can always be decomposed into the sum of three fundamental quantities: the `variance` of $\\hat{f}(x_0)$, the squared `bias` of $\\hat{f}(x_0)$ and the variance of the error terms $\\epsilon$. That is,\n",
    "\n",
    "$E(y_0-\\hat{f}(x_0))^2=Var(\\hat{f}(x_0))+[Bias(\\hat{f}(x_0))]^2+Var(\\epsilon)\\ (2.7)$\n",
    "\n",
    "Equation 2.7 tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves `low variance` and `low bias`.\n",
    "\n",
    "`Variance` refers to the amount by which $\\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\\hat{f}$. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in $\\hat{f}$. In general, more flexible statistical methods have higher variance. Consider the green and orange curves in Figure 2.9. The flexible green curve is following the observations very closely. It has high variance because changing any one of these data points may cause the estimate $\\hat{f}$ to change considerably. In contrast, the orange least squares line is relatively inflexible and has low variance, because moving any single observation will likely cause only a small shift in the position of the line.\n",
    "\n",
    "On the other hand, `bias` refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between Y and X1, X2, . . . , Xp. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f. In Figure 2.11, the true f is substantially non-linear, so no matter how many training observations we are given, it will not be possible to produce an accurate estimate using linear regression. In other words, linear regression results in high bias in this example. However, in Figure 2.10 the true f is very close to linear, and so given enough data, it should be possible for linear regression to produce an accurate estimate. Generally, more flexible methods result in less bias.\n",
    "\n",
    "As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. Note that we observed this pattern of decreasing test MSE followed by increasing test MSE in the right-hand panels of Figures 2.9–2.11.\n",
    "\n",
    "![2.12](http://ou8qjsj0m.bkt.clouddn.com//17-12-14/83715270.jpg)\n",
    "\n",
    "The relationship between bias, variance, and test set MSE given in Equa- tion 2.7 and displayed in Figure 2.12 is referred to as the `bias-variance trade-off`.\n",
    "\n",
    "### 2.2.3 The Classification Setting\n",
    "The most common approach for quantifying the accuracy of our estimate $\\hat{f}$ is the training `error rate`, the proportion of mistakes that are made if we apply our estimate $\\hat{f}$ to the training observations:\n",
    "\n",
    "$\\frac{1}{n}\\sum_{i=1}^n I(y_i \\ne \\hat{y_i})\\ (2.8)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
