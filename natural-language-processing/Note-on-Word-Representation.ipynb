{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note on Word Representation\n",
    "    Chao Yang\n",
    "    Microsoft\n",
    "    Suzhou yangchao.42@outlook.com\n",
    "\n",
    "https://github.com/placebokkk/writing/blob/master/word_embedding_note/word_representation_note_en.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1. 拥有相同context的word在语义上\u001b更相似，vector representation也\b更接近。\n",
    "1. 用矩阵M表征word与它的context的count关系。\n",
    "1. SVD\u001b分解矩阵可以得到W（word向量）和C（context向量）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Two kind of models:\n",
    "1. the count-based model\n",
    "1. the log-bilinear model\n",
    "    1. Skip-Gram\n",
    "    1. Glove\n",
    "\n",
    "All the count-based and log-bilinear models only use bag-of-words contexts information.\n",
    "\n",
    "### word-context matrix\n",
    "Each row is the representation of a word by its context counts. We use M count to notate this matrix.\n",
    "\n",
    "Table 1: The word-context count matrix $M^{count}$\n",
    "\n",
    "$$\n",
    " \\begin{bmatrix}\n",
    "   -, & c_1, & c_2, & \\cdots, & c_{|V_c|}\\\\\n",
    "   w_1 & 17, & 0, & \\cdots, & 1\\\\\n",
    "   w_2 & 1, & 42, & \\cdots, & 0\\\\\n",
    "   \\cdots, & \\cdots, & \\cdots,& \\cdots,& \\cdots,\\\\\n",
    "   w_{|V_w|} & 2, & 89, & \\cdots, & 0\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First, we make some assumption:\n",
    "\n",
    "- Any word w could be represented by a vector $\\vec{w}$ and any context could be represented by a vector $\\vec{c}$.\n",
    "- $\\vec{w} \\cdot \\vec{c}$ could be used as an association metric of the word w and context c.\n",
    "- The words having similar context will have the similar meaning, or close vector representation.\n",
    "\n",
    "Then for two words $w_i$ and $w_j$ , if they are similar in semantic space, for most context $c_k$, $\\vec{w_i} \\cdot \\vec{c_k}$ and $\\vec{w_j} \\cdot \\vec{c_k}$ will have similar value. If we put all the products $\\vec{w} \\cdot \\vec{c}$ into a matrix M , in which $M_{ik} =\\vec{w_i} \\cdot \\vec{c_k}$, we get\n",
    "\n",
    "$M=WC^T\\ (1)$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "W=\n",
    " \\begin{bmatrix}\n",
    "   \\vec{w_1}^T, \\\\\n",
    "   \\vec{w_2}^T, \\\\\n",
    "   \\cdots,\\\\\n",
    "   \\vec{w_{|V_w|}}^T\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$C^T=(\\vec{c_1},\\vec{c_2},\\cdots,\\vec{|V_c|})\\ (2)$$\n",
    "\n",
    "A good measure of the association of word and context:**PMI**\n",
    "\n",
    "$$PMI(w_i,c_k)=log\\frac{p(w_i,c_k)}{p(w_i)p(c_k)}\\ (3)$$\n",
    "\n",
    "$$PPMI(w_i,c_k)=max\\{PMI(w_i,c_k),0\\}$$\n",
    "\n",
    "We need to find a factorization of M to get W and C:\n",
    "\n",
    "$$W=M,C=\\Lambda\\ (5)$$\n",
    "\n",
    "In **SVD**, M will be first factorized into $U \\cdot \\Sigma \\cdot V^T$ , where U and V are orthonormal and Σ is a diagonal matrix of eigenvalues in decreasing order. By keeping only the top d elements of Σ, we obtain $U_d \\cdot \\Sigma_d \\cdot V_d^T$ . $M_d$ is a low-rank approximation of M.\n",
    "\n",
    "Now the words could be represented in a new linear space $W^{SVD} = U_d \\cdot \\Sigma_d$, spanned by a set of more compact basics $C^{SVD} = V_d$ .\n",
    "\n",
    "### Skip Gram Negative Sampling\n",
    "In mikolov’s paper, the objective function of Skip Gram is defined as:\n",
    "\n",
    "$$argmax_{\\theta}\\sum_{t=1}^T\\sum_{-l \\le j \\le l,j \\ne 0} log p(w_{t+j}|w_t)\\ (6)$$\n",
    "\n",
    "A more general formation:\n",
    "\n",
    "$$argmax_{\\theta}\\sum_{t=1}^T\\sum_{c' \\in Context(w_t)} log p(c'|w_t)\\ (7)$$\n",
    "\n",
    "The conditional probability of a context given its word is defined as:\n",
    "\n",
    "$$p(c|w)=\\frac{exp(\\vec{c}\\cdot\\vec{w})}{\\sum_{c' \\in V_c}exp(\\vec{c'}\\cdot\\vec{w})}\\ (8)$$\n",
    "\n",
    "**Neural view** (a three layer neural network):\n",
    "\n",
    "- Input layer with one-hot input.\n",
    "- Hidden layer with identity activation function.\n",
    "- Softmax output layer.\n",
    "- The transition between the input layer and hidden layer is the matrix W \n",
    "- The transition between the hidden layer and output layer is the matrix C\n",
    "\n",
    "**Optimization** The objective (6) could be computed by gradient descent directly. But it is expensive due to the summation $\\sum_{c' \\in V_c}exp(\\vec{c'}\\cdot\\vec{w})$. So Mikolov used an easier objective to replace the log p(c|w):\n",
    "\n",
    "$$log\\sigma(\\vec{c}\\cdot\\vec{w})+\\sum_{i=1}^k E_{c_i \\in P_n(w)}log \\sigma(-\\vec{c_i}\\cdot\\vec{w})\\ (9)$$\n",
    "\n",
    "$P_n(w)$ is the non-context probability distribution of word w. (9) change the output layer’s activation function form softmax to sigmoid. And at each iteration, it considers the current context and randomly use another k non-contexts (called as negative samples) to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
