{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method\n",
    "\n",
    "    Yoav Goldberg and Omer Levy\n",
    "    {yoav.goldberg,omerlevy}@gmail.com\n",
    "\n",
    "    February 14, 2014\n",
    "\n",
    "https://arxiv.org/pdf/1402.3722v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1. skip-gram model目标是极大似然估计得到w条件下的c（w，c分别代表当前词和该词上下文的word vector）。\n",
    "1. Negative Sampling：给定一个corpus D以及它的补集D'，优化它的极大似然估计，使得两个集合尽可能的分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The skip-gram model\n",
    "In this model we are given a corpus of words w and their contexts c. \n",
    "\n",
    "$argmax_{\\theta}\\prod_{w \\in Text}[\\prod_{c \\in C(w)}p(c|w;\\theta)]\\ (1)$\n",
    "\n",
    "- C(w) is the set of contexts of word w.\n",
    "\n",
    "$argmax_{\\theta}\\prod_{(w,c) \\in D}p(c|w;\\theta)\\ (2)$\n",
    "\n",
    "- D is the set of all word and context pairs we extract from the text.\n",
    "\n",
    "$p(c|w;\\theta)=\\frac{e^{v_c \\cdot v_w}}{\\sum_{c' \\in C e^{v_{c'} \\cdot v_w}}}\\ (3)$\n",
    "\n",
    "- $v_c$ and $v_w \\in R^d$ are vector representations for c and w respectively,\n",
    "- C is the set of all available contexts\n",
    "- the parameters $\\theta$ are $v_{c_i}, v_{w_i}$ for $w \\in V , c \\in C,i \\in 1,\\cdots,d$(a total of |C|×|V|×d parameters)\n",
    "\n",
    "Take the log and switch from product to sum:\n",
    "\n",
    "$argmax_{\\theta}\\sum_{(w,c) \\in D} log p(c|w)=\\sum_{(w,c) \\in D}(log e^{v_c \\cdot v_w}-log\\sum_{c'}e^{v_{c'} \\cdot v_w})\\ (4)$\n",
    "\n",
    "While objective (4) can be computed, it is computationally expensive to do so, because the term p(c|w; θ) is very expensive to compute due to the summation $\\sum_{c' \\in C}e^{v_{c'} \\cdot v_w}$ over all the contexts c′ (there can be hundreds of thousands of them). One way of making the computation more tractable is to replace the softmax with an **hierarchical softmax.**\n",
    "\n",
    "## Negative Sampling\n",
    "Consider a pair (w,c) of word and context.\n",
    "\n",
    "- p(D = 1|w, c) the probability that (w, c) came from the corpus data\n",
    "- p(D = 0|w, c) = 1 − p(D = 1|w, c) will be the probability that (w,c) did not come from the corpus data\n",
    "\n",
    "Our goal is now to find parameters to maximize the probabilities that all of the observations indeed came from the data:\n",
    "\n",
    "$argmax_{\\theta}\\prod_{(w,c) \\in D}p(D=1|w,c;\\theta)$\n",
    "\n",
    "$=argmax_{\\theta}log\\prod_{(w,c) \\in D}p(D=1|w,c;\\theta)$\n",
    "\n",
    "$=argmax_{\\theta}\\sum_{(w,c) \\in D}log p(D=1|w,c;\\theta)$\n",
    "\n",
    "The quantity p(D = 1|c,w; θ) can be defined using softmax:\n",
    "\n",
    "$p(D=1|w,c;\\theta)=\\frac{1}{1+e^{-v_c \\cdot v_w}}$\n",
    "\n",
    "Leading to the objective:\n",
    "\n",
    "$argmax_{\\theta}\\sum_{(w,c) \\in D}log\\frac{1}{1+e^{-v_c \\cdot v_w}}$\n",
    "\n",
    "We need a mechanism that prevents all the vectors from having the same value, by disallowing some (w, c) combinations. One way to do so, is to present the model with some (w,c) pairs for which p(D = 1|w,c;θ) must be low, i.e. pairs which are not in the data. This is achieved by generating the set D′ of random (w,c) pairs, assuming they are all incorrect (the name “negative sampling” stems from the set D′ of randomly sampled negative examples).\n",
    "\n",
    "$argmax_{\\theta}\\prod_{(w,c) \\in D}p(D=1|c,w;\\theta)\\prod_{(w,c) \\in D'}p(D=0|c,w;\\theta)$\n",
    "\n",
    "$=argmax_{\\theta}\\prod_{(w,c) \\in D}p(D=1|c,w;\\theta)\\prod_{(w,c) \\in D'}(1-p(D=1|c,w;\\theta))$\n",
    "\n",
    "$=argmax_{\\theta}\\sum_{(w,c) \\in D}log p(D=1|c,w;\\theta)+\\sum_{(w,c) \\in D'}log(1-p(D=1|c,w;\\theta))$\n",
    "\n",
    "$=argmax_{\\theta}\\sum_{(w,c) \\in D}log\\frac{1}{1+e^{-v_c \\cdot v_w}}+\\sum_{(w,c) \\in D'}log(1-\\frac{1}{1+e^{-v_c \\cdot v_w}})$\n",
    "\n",
    "$=argmax_{\\theta}\\sum_{(w,c) \\in D}log\\frac{1}{1+e^{-v_c \\cdot v_w}}+\\sum_{(w,c) \\in D'}log\\frac{1}{1+e^{v_c \\cdot v_w}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
