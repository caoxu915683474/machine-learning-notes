{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类：寻找相似的帖子\n",
    "- 词袋模型与文本向量化\n",
    "- 相似度：向量间距离\n",
    "- 聚类KMeans\n",
    "\n",
    "《机器学习系统设计》第三章\n",
    "\n",
    "## 评估帖子关联性\n",
    "- 对每个帖子提取重要特征，并针对每个帖子存储为一个向量。\n",
    "- 在这些向量上进行聚类计算。\n",
    "- 确定每个待聚类帖子所在的簇。\n",
    "- 对每个簇。获取几个与待聚类帖子不同的帖子。这样可以提升多样性。\n",
    "\n",
    "## 预处理：用相近的公共词语个数来衡量相似性\n",
    "文本处理的步骤：\n",
    "- 切分文本：将原始文本转化为词袋\n",
    "- 扔掉出现过于频繁，而又对检测相关贴子没有帮助的词语\n",
    "- 扔掉出现频率很低，只有很小可能出现在未来帖子中的词语\n",
    "- 统计剩余的词语\n",
    "- 考虑整个语料集合，从词频统计中计算TF-IDF值\n",
    "\n",
    "词袋模型缺点：\n",
    "- 不涵盖词语之间的关联关系。\"Car hits wall\"和\"Wall hits car\"\n",
    "- 没法正确捕捉否定关系。\"I will eat ice cream\"和\"I will not eat ice cream\"从特征向量来看非常相似。解决方法使用unigrams(单个词语),bigrams(成对的词语)或者trigrams(一行中的三个词语)\n",
    "- 对于拼写错误的词语会处理失败。\n",
    "\n",
    "### 导入语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "Imaging databases provide storage capabilities.\n",
      "Most imaging databases save images permanently.\n",
      "Imaging databases store data.\n",
      "Imaging databases store data. Imaging databases store data. Imaging databases store data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "DATA_DIR = os.path.join('../', \"data\")\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    print(\"Uh, we were expecting a data directory, which contains the toy data\")\n",
    "    sys.exit(1)\n",
    "\n",
    "CHART_DIR = os.path.join('../', \"charts\")\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)\n",
    "\n",
    "TOY_DIR = os.path.join(DATA_DIR, \"toy\")\n",
    "posts = [open(os.path.join(TOY_DIR, f)).read() for f in os.listdir(TOY_DIR)]\n",
    "for p in posts:\n",
    "    print p.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "当某个词语经常出现在一些特定帖子中，而在其他地方很少出现，会给该词语较高的权值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.270310072072\n",
      "0.0\n",
      "0.135155036036\n",
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "\n",
    "\n",
    "def tfidf(t, d, D):  # term, doc of term, doc set\n",
    "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\n",
    "    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\n",
    "    return tf * idf\n",
    "\n",
    "\n",
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "\n",
    "print(tfidf(\"a\", a, D))\n",
    "print(tfidf(\"b\", abb, D))\n",
    "print(tfidf(\"a\", abc, D))\n",
    "print(tfidf(\"b\", abc, D))\n",
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===stemmedCountVectorizer===\n",
      "#samples: 5, #features: 17\n",
      "(<1x17 sparse matrix of type '<type 'numpy.int64'>'\n",
      "\twith 2 stored elements in Compressed Sparse Row format>, <class 'scipy.sparse.csr.csr_matrix'>)\n",
      "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.63: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.77: Imaging databases store data.\n",
      "=== Post 4 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.63\n",
      "===stemmedTfidfVectorizer===\n",
      "#samples: 5, #features: 17\n",
      "(<1x17 sparse matrix of type '<type 'numpy.float64'>'\n",
      "\twith 2 stored elements in Compressed Sparse Row format>, <class 'scipy.sparse.csr.csr_matrix'>)\n",
      "[[ 0.          0.          0.          0.          0.70710678  0.70710678\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'save', u'storag', u'store', u'stuff', u'toy']\n",
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=0.86: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=0.92: Imaging databases store data.\n",
      "=== Post 4 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 2 with dist=0.86\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')  # 词干处理\n",
    "\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "\n",
    "def train(vectorizer):\n",
    "    X_train = vectorizer.fit_transform(posts)  # 向量化\n",
    "\n",
    "    num_samples, num_features = X_train.shape\n",
    "    print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "    new_post_vec = vectorizer.transform([new_post])\n",
    "    print(new_post_vec, type(new_post_vec))\n",
    "    print(new_post_vec.toarray())\n",
    "    print(vectorizer.get_feature_names())\n",
    "    return X_train, new_post_vec\n",
    "\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())  # 向量的2范数\n",
    "\n",
    "\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())  # 向量归一化\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "\n",
    "    delta = v1_normalized - v2_normalized\n",
    "\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "\n",
    "def evaluate(model, new_post_vec):\n",
    "    best_dist = sys.maxsize\n",
    "    best_i = None\n",
    "    num_samples, num_features = model.shape\n",
    "\n",
    "    for i in range(0, num_samples):\n",
    "        post = posts[i]\n",
    "        if post == new_post:\n",
    "            continue\n",
    "        post_vec = model.getrow(i)\n",
    "        d = dist(post_vec, new_post_vec)\n",
    "\n",
    "        print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "\n",
    "    print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))\n",
    "\n",
    "new_post = \"imaging databases\"\n",
    "\n",
    "# vectorizer = CountVectorizer(min_df=1, stop_words='english', preprocessor=stemmer)\n",
    "stemmedCountVectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')\n",
    "stemmedTfidfVectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english', decode_error='ignore')\n",
    "dist = dist_norm\n",
    "\n",
    "print '===stemmedCountVectorizer==='\n",
    "model, new_post_vec = train(stemmedCountVectorizer)\n",
    "evaluate(model, new_post_vec)\n",
    "\n",
    "print '===stemmedTfidfVectorizer==='\n",
    "model, new_post_vec = train(stemmedTfidfVectorizer)\n",
    "evaluate(model, new_post_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 聚类\n",
    "20newsgroup数据集是机器学习中的一个标准数据集。来自不同新闻组。我们选取一部分作为训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total posts: 18846\n",
      "('Number of training posts in tech groups:', 3529)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import sklearn.datasets\n",
    "all_data = sklearn.datasets.fetch_20newsgroups(subset=\"all\")\n",
    "print(\"Number of total posts: %i\" % len(all_data.filenames))\n",
    "\n",
    "groups = [\n",
    "    'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware',\n",
    "    'comp.sys.mac.hardware', 'comp.windows.x', 'sci.space']\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(subset=\"train\",\n",
    "                                                 categories=groups)\n",
    "print(\"Number of training posts in tech groups:\", len(train_data.filenames))\n",
    "# Number of training posts in tech groups: 3529\n",
    "\n",
    "labels = train_data.target\n",
    "sp.unique(labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集属于六个组，所以我们定义kmeans的簇为6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_clusters = 6  # sp.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对帖子做向量化预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 3529, #features: 4712\n"
     ]
    }
   ],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
    "                                    stop_words='english', decode_error='ignore'\n",
    "                                    )\n",
    "\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))  # row 3529， column 4712"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对帖子做聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 6511.359\n",
      "Iteration  1, inertia 3400.718\n",
      "Iteration  2, inertia 3385.624\n",
      "Iteration  3, inertia 3378.951\n",
      "Iteration  4, inertia 3375.098\n",
      "Iteration  5, inertia 3373.139\n",
      "Iteration  6, inertia 3371.640\n",
      "Iteration  7, inertia 3370.683\n",
      "Iteration  8, inertia 3369.839\n",
      "Iteration  9, inertia 3369.309\n",
      "Iteration 10, inertia 3369.086\n",
      "Iteration 11, inertia 3368.998\n",
      "Iteration 12, inertia 3368.950\n",
      "Iteration 13, inertia 3368.927\n",
      "Converged at iteration 13\n",
      "km.labels_=[1 1 1 ..., 3 3 0]\n",
      "km.labels_.shape=3529\n",
      "Homogeneity: 0.211\n",
      "Completeness: 0.262\n",
      "V-measure: 0.233\n",
      "Adjusted Rand Index: 0.109\n",
      "Adjusted Mutual Information: 0.209\n",
      "Silhouette Coefficient: 0.006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_init=1, verbose=1, random_state=3)\n",
    "clustered = km.fit(vectorized)\n",
    "\n",
    "print(\"km.labels_=%s\" % km.labels_)\n",
    "# km.labels_=[[1 1 1 ..., 3 3 0]] 6个cluster,由id=0,1,...,5代表\n",
    "\n",
    "print(\"km.labels_.shape=%s\" % km.labels_.shape)\n",
    "# km.labels_.shape=3529 这3529个点分别属于哪个cluster\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels, km.labels_))\n",
    "print((\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(vectorized, labels, sample_size=1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新的帖子做向量化，并给出聚类结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = \\\n",
    "    \"\"\"Disk drive problems. Hi, I have a problem with my hard disk.\n",
    "After 1 year it is working only sporadically now.\n",
    "I tried to format it, but now it doesn't boot any more.\n",
    "Any ideas? Thanks.\n",
    "\"\"\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]  # 5 意味着和cluster5相似\n",
    "new_post_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然有了聚类信息，我们不必要用new_post_vec和所有帖子的向量进行比较，相反，我们只需要专注于同一个簇中的帖子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "similar_indices = (km.labels_ == new_post_label).nonzero()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "括号中比较操作得到一个布尔型数组，nonzero将这个数组转化为一个更小的数组，它包含True元素的索引。使用similar_indices构建一个帖子列表，以及它们的相似度分值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count similar: 226\n",
      "=== #1 ===\n",
      "(1.0378441731334074, u\"From: Thomas Dachsel <GERTHD@mvs.sas.com>\\nSubject: BOOT PROBLEM with IDE controller\\nNntp-Posting-Host: sdcmvs.mvs.sas.com\\nOrganization: SAS Institute Inc.\\nLines: 25\\n\\nHi,\\nI've got a Multi I/O card (IDE controller + serial/parallel\\ninterface) and two floppy drives (5 1/4, 3 1/2) and a\\nQuantum ProDrive 80AT connected to it.\\nI was able to format the hard disk, but I could not boot from\\nit. I can boot from drive A: (which disk drive does not matter)\\nbut if I remove the disk from drive A and press the reset switch,\\nthe LED of drive A: continues to glow, and the hard disk is\\nnot accessed at all.\\nI guess this must be a problem of either the Multi I/o card\\nor floppy disk drive settings (jumper configuration?)\\nDoes someone have any hint what could be the reason for it.\\nPlease reply by email to GERTHD@MVS.SAS.COM\\nThanks,\\nThomas\\n+-------------------------------------------------------------------+\\n| Thomas Dachsel                                                    |\\n| Internet: GERTHD@MVS.SAS.COM                                      |\\n| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\\n| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\\n| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\\n| Fax:      +49 6221 415101                                         |\\n| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\\n| Tagline:  One bad sector can ruin a whole day...                  |\\n+-------------------------------------------------------------------+\\n\")\n",
      "()\n",
      "=== #2 ===\n",
      "(1.1932822796029161, u'From: Ravi Konchigeri <mongoose@leland.stanford.edu>\\nSubject: Re: LCIII problems\\nX-Xxmessage-Id: <A7F4A76B690100ED@kimball-pc-316.stanford.edu>\\nX-Xxdate: Fri, 16 Apr 93 02:11:55 GMT\\nOrganization: Stanford University\\nX-Useragent: Nuntius v1.1.1d17\\nLines: 24\\n\\nIn article <1qmgjk$ao5@menudo.uh.edu> , sunnyt@coding.bchs.uh.edu writes:\\n>\\tIts not a good idea to have a horizontally formatted hard disk in a  \\n>vertical position.  If the drive is formatted in a horizontal position,\\nit can  \\n>not completely compensate for the gravitational pull in a vertical\\nposition.   \\n>I\\'m not saying that your hard disk will fail tomorrow or 6 months from\\nnow, but  \\n>why take that chance?  If you want more detailed info on the problem,\\nplease  \\n\\nI think the other replies sum up the fact that you can place a hard drive\\non its side.  The point is this will only be sure to work on the \\'new\\'\\ndrives, namely 1/3 ht LPS drives that have a smaller platter and are also\\nmore stable.\\n\\tWhy should I take the chance?  Because I\\'ve been running a Maxtor 1/3 ht\\n120 LPS on both its side and flat for about a year and I\\'ve had no\\nproblems with it.  Period.\\n\\tLike I always say, NEVER trust the manufacturer.\\n\\n\\t\"Just like everything else in life, the right lane ends in half a mile.\"\\n\\nRavi Konchigeri.\\nmongoose@leland.stanford.edu\\n')\n",
      "()\n",
      "=== #3 ===\n",
      "(1.3263470648788591, u\"From: narain@ih-nxt09.cso.uiuc.edu (Nizam Arain)\\nSubject: Floptical Question\\nArticle-I.D.: news.C519MM.M2L\\nReply-To: narain@uiuc.edu\\nOrganization: University of Illinois at Urbana\\nLines: 17\\n\\nHi. I am looking into buying a Floptical Drive, and was wondering what  \\nexperience people have with the drives from Iomega, PLI, MASS MicroSystems,  \\nor Procom. These seem to be the main drives on the market. Any advice?\\n\\nAlso, I heard about some article in MacWorld (Sep '92, I think) about  \\nFlopticals. Could someone post a summary, if they have it?\\n\\nThanks in advance. (Reply by post or email, whichever you prefer.)\\n\\n--Nizam\\n\\n--\\n\\n /  *  \\\\   Nizam Arain                           \\\\ What makes the universe\\n||     ||  (217) 384-4671                        / so hard to comprehend \\n| \\\\___/ |  Internet: narain@uiuc.edu             \\\\ is that there is nothing\\n \\\\_____/   NeXTmail: narain@sumter.cso.uiuc.edu  / to compare it with.\\n\")\n"
     ]
    }
   ],
   "source": [
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, train_data.data[i]))\n",
    "\n",
    "similar = sorted(similar)\n",
    "print(\"Count similar: %i\" % len(similar))\n",
    "\n",
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[int(len(similar) / 10)]\n",
    "show_at_3 = similar[int(len(similar) / 2)]\n",
    "\n",
    "print(\"=== #1 ===\")\n",
    "print(show_at_1)\n",
    "print()\n",
    "\n",
    "print(\"=== #2 ===\")\n",
    "print(show_at_2)\n",
    "print()\n",
    "\n",
    "print(\"=== #3 ===\")\n",
    "print(show_at_3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ipykernel_py2]",
   "language": "python",
   "name": "Python [ipykernel_py2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
