{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最大熵分类器\n",
    "没有使用概率设置模型的参数，而是使用搜索技术找出一组能最大限度地提高分类器性能的参数。特别的，它查出能使训练语料的`整体似然性`(`total likelihood`)最大的参数组。其定义如下\n",
    "\n",
    "$(10) P(features)=\\sum_{x \\in corpus}P(label(x)|features(x))$\n",
    "\n",
    "其中P(label|features)，即特征为features的输入且类标签为label的概率，被定义为：\n",
    "\n",
    "$(11) P(label|features)=\\frac{P(label, features)}{\\sum_{label}P(label, features)}$\n",
    "\n",
    "最大熵分类器采用迭代优化(`iterative optimization`)技术选择模型参数，该技术利用随机值初始化模型的参数，然后反复优化这些参数，使它们更接近最优解。\n",
    "\n",
    "## 最大熵模型\n",
    "每个接收它自身参数的标签和特征的组合被称为联合特征(`joint-feature`)。联合特征是加标签值的属性，而（简单）特征是未加标签值的属性。\n",
    "\n",
    "每个标签定义的联合特征对应于w[label]，每个（简单）特征和标签组合定义的联合特征对应于w[f,label]。给定一个最大熵模型的联合特征，分配到给定输入标签的得分仅仅是适用于该输入和标签的联合特征与参数之间的简单乘积。\n",
    "\n",
    "$(12) P(input, label)=\\prod_{joint-features(input,label)}w[joint-feature]$\n",
    "\n",
    "## 熵的最大化\n",
    "假设从10个可能词意列表（标签从A-J）中为一个给定的词找出正确词意。首先，我们没有被告知其他任何关于词或词意的信息。我们可以为10种词意选择的概率分布很多，例如：\n",
    "\n",
    "| - | A |  B | C | D | E | F | G | H | I | J |\n",
    "| - | - |  - | - | - | - | - | - | - | - | - |\n",
    "| (i) | 10% | 10% | 10% | 10% | 10% | 10% | 10% | 10% | 10% | 10% |\n",
    "| (ii) | 5% | 15% | 0% | 30% | 0% | 8% | 12% | 0% | 6% | 24% |\n",
    "| (iii) | 0% | 100% | 0% | 0% | 0% | 0% | 0% | 0% | 0% | 0% |\n",
    "\n",
    "虽然这些分布都有可能是正确的，但我们最可能会选择的是分布（i），因为没有任何更多的信息，也没有理由相信任何词的词意比其他的更有可能。\n",
    "\n",
    "如果是一个单独的标签则熵较低，但如果标签的分布比较均匀则熵较高。最大熵原理(`Maximum Entropy principle`)是指在已知的分布下，我们会选择熵最高的分布。\n",
    "\n",
    "接下来，假设被告知词意A出现的次数占55%。还有许多分布适合于这条信息，例如：\n",
    "\n",
    "| - | A |  B | C | D | E | F | G | H | I | J |\n",
    "| - | - |  - | - | - | - | - | - | - | - | - |\n",
    "| (iv) | 55% | 45% | 0% | 0% | 0% | 0% | 0% | 0% | 0% | 0% |\n",
    "| (v) | 55% | 5% | 5% | 5% | 5% | 5% | 5% | 5% | 5% | 5% |\n",
    "| (vi) | 55% | 3% | 1% | 2% | 9% | 5% | 0% | 25% | 0% | 0% |\n",
    "\n",
    "最后，假设被告知词up出现在附近上下文中的次数占10%，当它出现在这个上下文中时有80%的可能使用词意A或C。\n",
    "\n",
    "| - | - | A |  B | C | D | E | F | G | H | I | J |\n",
    "| - | - | - |  - | - | - | - | - | - | - | - | - |\n",
    "| (vii) | +up | 5.1% | 0.25% | 2.9% | 0.25% | 0.25% | 0.25% | 0.25% | 0.25% | 0.25% | 0.25% |\n",
    "| - | -up | 49.9% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% | 4.46% |\n",
    "\n",
    "特别地，这个分布与我们所知道的一致：如果我们将A列的概率加起来，是55%，如果我们将第1行的概率加起来，是10%；如果我们将+up行词意A和C的概率加起来，是8%（或+up情况的80%）。此外，其余的概率好像是“均匀分布”的。\n",
    "\n",
    "我们将自己限制在已知的分布上。其中，我们选择最高熵的分布。特别地，对于每个联合特征，最大熵模型计算该特征的“经验频率”——即它出现在训练集中的频率。然后，搜索能使熵最大的分布，同时预测每个联合特征正确的概率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
