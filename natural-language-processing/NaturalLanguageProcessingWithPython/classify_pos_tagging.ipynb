{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类文本词性标注\n",
    "词性标注(`Part-of-Speech Tagging`):训练一个分类器来算出哪个后缀最有信息量。首先，让我们找出最常见的后缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'vre',\n",
       " u'g/l',\n",
       " u'mny',\n",
       " u'.32',\n",
       " u\"k's\",\n",
       " u'cwt',\n",
       " u'cth',\n",
       " u'*yt',\n",
       " u'ajk',\n",
       " u'pce',\n",
       " u\"y'n\",\n",
       " u'sch',\n",
       " u'.e.',\n",
       " u'cm.',\n",
       " u'xts',\n",
       " u'aud',\n",
       " u'rek',\n",
       " u'aui',\n",
       " u'/3%',\n",
       " u'oeb',\n",
       " u'pth',\n",
       " u'aum',\n",
       " u'aul',\n",
       " u'xty',\n",
       " u'aun',\n",
       " u'sce',\n",
       " u'aus',\n",
       " u'aur',\n",
       " u'aut',\n",
       " u'343',\n",
       " u'-ho',\n",
       " u'aux',\n",
       " u'$40',\n",
       " u'348',\n",
       " u'yms',\n",
       " u'$45',\n",
       " u'xth',\n",
       " u'mns',\n",
       " u'sca',\n",
       " u'hce',\n",
       " u'.k.',\n",
       " u'*yr',\n",
       " u'606',\n",
       " u'fur',\n",
       " u'nw.',\n",
       " u'ala',\n",
       " u'lbs',\n",
       " u'aix',\n",
       " u'upi',\n",
       " u'29%',\n",
       " u'fha',\n",
       " u'upa',\n",
       " u'nmr',\n",
       " u'298',\n",
       " u'upy',\n",
       " u'297',\n",
       " u'upt',\n",
       " u'ups',\n",
       " u'290',\n",
       " u'bl',\n",
       " u'27%',\n",
       " u'*yp',\n",
       " u'ab/',\n",
       " u'270',\n",
       " u'271',\n",
       " u'ucy',\n",
       " u'273',\n",
       " u'274',\n",
       " u'275',\n",
       " u'276',\n",
       " u'3a',\n",
       " u'7th',\n",
       " u'air',\n",
       " u'ptu',\n",
       " u'nen',\n",
       " u'ghn',\n",
       " u'nel',\n",
       " u'zur',\n",
       " u'nek',\n",
       " u'oze',\n",
       " u'nei',\n",
       " u'ned',\n",
       " u'nee',\n",
       " u'kus',\n",
       " u'nec',\n",
       " u'kup',\n",
       " u'jac',\n",
       " u'ain',\n",
       " u'nez',\n",
       " u'ak.',\n",
       " u'nex',\n",
       " u'ney',\n",
       " u'new',\n",
       " u'net',\n",
       " u'neu',\n",
       " u'ner',\n",
       " u'nes',\n",
       " u'mee',\n",
       " u'med',\n",
       " u'meg']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "\n",
    "common_suffixes = suffix_fdist.keys()[:100]\n",
    "common_suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个特征提取函数，检查给定单词的后缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义特征提取器，用它来训练新的“[决策树](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/ch06s04.html)”的分类器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14639482844356042"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'NN'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(pos_features('cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型的优点是容易解释。可以指示NLTK将它们以伪代码形式输出。\n",
    "\n",
    "```py\n",
    ">>> print classifier.pseudocode(depth=4)\n",
    "if endswith(,) == True: return ','\n",
    "if endswith(,) == False:\n",
    "  if endswith(the) == True: return 'AT'\n",
    "  if endswith(the) == False:\n",
    "    if endswith(s) == True:\n",
    "      if endswith(is) == True: return 'BEZ'\n",
    "      if endswith(is) == False: return 'VBZ'\n",
    "    if endswith(s) == False:\n",
    "      if endswith(.) == True: return '.'\n",
    "      if endswith(.) == False: return 'NN'\n",
    "```\n",
    "\n",
    "### 探索上下文语境\n",
    "语境特征往往提供关于正确标记的强大线索——例如：标注词fly时，如果知道它前面的词是“a”，能够确定它是名词，而不是动词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev-word': u'an',\n",
       " 'suffix(1)': u'n',\n",
       " 'suffix(2)': u'on',\n",
       " 'suffix(3)': u'ion'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "pos_features(brown.sents()[0], 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7891596220785678"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append((pos_features(untagged_sent, i), tag))\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 序列分类\n",
    "为了获取相关分类任务之间的依赖关系，我们可以使用`joint classifier`模型，为一些相关的输入选择适当的标签。在词性标注的例子中，可以使用各种不同的`sequence classifier`模型为给定的句子中的所有词选择词性标签。\n",
    "\n",
    "一种称为`consecutive classification`或`greedy sequence classification`的序列分类器策略，为第一个输入找到最有可能的类标签，然后在此基础上找到下一个输入的最佳的标签。这个过程可以不断重复直到所有的输入都被贴上标签。\n",
    "\n",
    "使用连续分类器进行词性标注。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7980528511821975"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他序列分类方法\n",
    "这种方法的缺点是一旦做出决定便无法更改。例如：如果决定将一个词标注为名词，但后来发现应该是动词，那也没有办法修复我们的错误了。解决这个问题的方法是采取转型策略(`transformational strategy`)。转型联合分类(`Transformational joint classifiers`)的工作原理是为输入的标签创建一个初始值，然后反复提炼该值，尝试修复相关输入之间的不一致。Brill标注器，是使用这种策略的。\n",
    "\n",
    "另一种方案是为词性标记所有可能的序列打分，选择总得得分最高的序列。隐马尔可夫模型(`Hidden Markov Models`)采取了这种方法。隐马尔可夫模型类似于连续分类器，不光考虑输入也考虑已预测标记的历史。然而，不是简单地找出一个给定词的单个最好标签，而是为标记产生一个概率分布。然后将这些概率结合起来计算标记序列的概率得分，最后选择最高概率的标记序列。不过，可能的标签序列数量相当大。给定拥有30个标签的标记集，大约有600万亿(30^10)种方式来标记一个10个词的句子。为了避免单独考虑所有这些可能的序列，隐马尔可夫模型要求特征提取器只考虑最近的标记来有效地找出最有可能的标记序列。特别是，对每个连续的词索引i，当前的及以前的每个可能的标记都将计算得分。这种基础的方法被两个更先进的模型所采用，它们被称为最大熵隐马尔可夫模型(`Maximum Entropy Markov Models`)和线性链条件随机场模型(`Linear-Chain Conditional Random Field Models`)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
