{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯分类器\n",
    "![6-6](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302076.png)\n",
    "\n",
    "Figure 6-6. An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document. In the training corpus, most documents are automotive, so the classifier starts out at a point closer to the “automotive” label. But it then considers the effect of each feature. In this example, the input document contains the word dark, which is a weak indicator for murder mysteries, but it also contains the word football, which is a strong indicator for sports documents. After every feature has made its contribution, the classifier checks which label it is closest to, and assigns that label to the input.\n",
    "\n",
    "![6-7](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302078.png)\n",
    "\n",
    "Figure 6-7. Calculating label likelihoods with naive Bayes. Naive Bayes begins by calculating the prior probability of each label, based on how frequently each label occurs in the training data. Every feature then contributes to the likelihood estimate for each label, by multiplying it by the probability that input values with that label will have that feature. The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features, assuming that the feature probabilities are all independent.\n",
    "\n",
    "![6-8](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302080.png)\n",
    "\n",
    "Figure 6-8. A Bayesian Network Graph illustrating the generative process that is assumed by the naive Bayes classifier. To generate a labeled input, the model first chooses a label for the input, and then it generates each of the input’s features based on that label. Every feature is assumed to be entirely independent of every other feature, given the label.\n",
    "\n",
    "## 潜在概率模型\n",
    "它为输入选择最有可能的标签，基于下面的假设：每个输入值是通过首先为该输入值选择类标签，然后产生每个特征的方式产生的，每个特征与其他特征完全独立。\n",
    "\n",
    "基于这个假设，我们可以计算表达式P(label|features)，即一个具有特定标签（假定具有特定的特征集）的输入概率。要为新输入选择标签，可以简单地选择能使P（l|features)最大的标签l。\n",
    "\n",
    "首先，注意到P(label|features)等于具有特定标签和特定特征集的输入的概率除以具有特定特征集的输入的概率。\n",
    "\n",
    "$(2) P(label|features)=\\frac{P(features,label)}{P(features)}$\n",
    "\n",
    "接下来，注意到P(features)对每个标签都是相同的。因此，如果我们只是对寻找最有可能的标签感兴趣，只需计算P(features,label)就够了，我们称之为似然(`likelihood`)标签。\n",
    "\n",
    "如果我们想生成每个标签的概率估计，而不是只选择最有可能的标签，那么计算P(features)的最简单的方法是仅仅计算P(features,label)在所有标签上的总和。\n",
    "\n",
    "$(3) P(features)=\\sum_{label \\in labels}P(features,label)$\n",
    "\n",
    "似然标签可以展开为标签的概率乘以给定标签特征的概率。\n",
    "\n",
    "$(4) P(features,label)=P(label) \\times P(features|label)$\n",
    "\n",
    "由于特征都是独立的（给定标签），可以分离每个独立特征的概率。\n",
    "\n",
    "$(5) P(features,label)=P(label) \\times \\prod_{f \\in features}P(f|label)$\n",
    "\n",
    "P(label)是一个给定标签的先验概率，每个P(f|label)是单个特征对标签可能性的贡献。\n",
    "\n",
    "## 零计数和平滑\n",
    "计算P(f|label)最简单的方法，特征f对标签label的标签似然性的贡献，是取得具有给定特征和给定标签的训练实例百分比。\n",
    "\n",
    "$(6) P(f|label)=\\frac{count(f,label)}{count(label)}$\n",
    "\n",
    "当count(f)变小时这个估计变得不可靠。因此，建立朴素贝叶斯模型时，采用平滑(`smoothing`)技术，用于计算P(f|label)，在给定标签的特征的概率的基础上。例如：给定标签的特征概率的期望似然估计(`Expected Likelihood Estimation`)基本上会给每个count(f,label)值增加0.5，Heldout估计(`Heldout Estimation`)使用heldout语料库计算特征频率与特征概率之间的关系。nltk.probability模块提供了多种平滑模块技术支持。\n",
    "\n",
    "## 非二元特征\n",
    "标签值特征（例如颜色）可以转化为二元（红色、非红色）。数字特征可以通过装箱(`binning`)转换为二元特征。\n",
    "\n",
    "另一种方法是通过回归方法来模拟数字特征的概率。\n",
    "\n",
    "## 双重计数的原因\n",
    "双重计数问题出现的原因是在训练过程中特征的贡献被分开计算，但当使用分类器为新输入选择标签时，这些特征的贡献被组合在一起了。因此，解决方案是考虑在训练中特征的贡献之间可能的相互作用。\n",
    "\n",
    "我们可以重写用于计算标签似然性的方程，分离出每个功能（或标签）所作出的贡献。\n",
    "\n",
    "$(7) P(features, label)=w[label] \\times \\prod_{f \\in features}w[f,label]$\n",
    "\n",
    "这里，w[label]是一个给定标签的“初始分数”，w[f,label]是给定特征对一个标签的似然性所作的贡献。我们称这些值w[label]和w[f,label]为模型的参数(`parameters`)或权重(`weights`)。使用朴素贝叶斯算法，我们单独设置这些参数。\n",
    "\n",
    "$(8) w[label]=P(label)$\n",
    "\n",
    "$(9) w[f,label]=P(f|label)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
