{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从文本提取信息\n",
    "## 信息提取\n",
    "### 信息提取结构\n",
    "![7-1](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302082.png)\n",
    "\n",
    "Figure 7-1. Simple pipeline architecture for an information extraction system. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple ([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']).\n",
    "\n",
    "要执行前面3个任务，需要定义一个函数，简单地连接NLTK中默认的句子分割器，分词器和词性标注器。\n",
    "\n",
    "```py\n",
    ">>> def ie_preprocess(document):\n",
    "...    sentences = nltk.sent_tokenize(document)\n",
    "...    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "...    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "```\n",
    "\n",
    "## 分块(Chunking)\n",
    "用于实体识别的基本技术是分块(`chunking`)，分割和标注如图7-2所示的多标识符序列。小框显示词级标识符和词性标注，同时，大框显示较高级别的程序分块。较大的框叫做组块(`chunk`)。\n",
    "\n",
    "![7-2](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302084.png)\n",
    "\n",
    "Figure 7-2. Segmentation and labeling at both the Token and Chunk levels.\n",
    "\n",
    "### 名词短语分块\n",
    "名词短语分块(`noun phrase chunking`)或NP-分块(`NP-chunking`)，寻找单独名词短语对应的块。下面文本，其中的NP-分块用方括号标记。\n",
    "\n",
    "```\n",
    "[ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/IN [ Digital/NNP ] [ ’s/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ] should/MD do/VB well/RB there/RB ./.\n",
    "```\n",
    "\n",
    "NP-分块信息最有用的来源之一是词性标注。\n",
    "\n",
    "```py\n",
    ">>> sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), # 已经标注词性的例句\n",
    "... (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    ">>> grammar = \"NP: {<DT>?<JJ>*<NN>}\" # 正则表达式定义一个语法：NP-分块由可选的且后面跟着任意数目形容词(JJ)的限定词(DJ)和名词(NN)组成\n",
    "\n",
    ">>> cp = nltk.RegexpParser(grammar) # 使用此语法创建组块分析器\n",
    ">>> result = cp.parse(sentence) # 测试例句\n",
    ">>> print result # 输出\n",
    "(S\n",
    "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
    "  barked/VBD\n",
    "  at/IN\n",
    "  (NP the/DT cat/NN))\n",
    ">>> result.draw()\n",
    "```\n",
    "\n",
    "![1](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302086.png)\n",
    "\n",
    "### 用正则表达式分块\n",
    "下例由2个规则组成的简单的分块语法。\n",
    "\n",
    "```py\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and nouns\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"), 1\n",
    "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    ">>> print cp.parse(sentence) 1\n",
    "(S\n",
    "  (NP Rapunzel/NNP)\n",
    "  let/VBD\n",
    "  down/RP\n",
    "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n",
    "```\n",
    "\n",
    "### 探索文本语料库\n",
    "使用分块器可以更轻松地完成这项工作：\n",
    "\n",
    "```py\n",
    ">>> cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
    ">>> brown = nltk.corpus.brown\n",
    ">>> for sent in brown.tagged_sents():\n",
    "...     tree = cp.parse(sent)\n",
    "...     for subtree in tree.subtrees():\n",
    "...         if subtree.node == 'CHUNK': print subtree\n",
    "...\n",
    "(CHUNK combined/VBN to/TO achieve/VB)\n",
    "(CHUNK continue/VB to/TO place/VB)\n",
    "(CHUNK serve/VB to/TO protect/VB)\n",
    "(CHUNK wanted/VBD to/TO wait/VB)\n",
    "(CHUNK allowed/VBN to/TO place/VB)\n",
    "(CHUNK expected/VBN to/TO become/VB)\n",
    "...\n",
    "(CHUNK seems/VBZ to/TO overtake/VB)\n",
    "(CHUNK want/VB to/TO buy/VB)\n",
    "```\n",
    "\n",
    "### 缝隙(Chinking)\n",
    "有时可以定义我们想从块中排除什么。可以为不包括在大块中的标识符序列定义一个缝隙(`Chinking`)。在下面的例子中，`barker/VBD at/IN`是一个缝隙。\n",
    "\n",
    "```\n",
    "[ the/DT little/JJ yellow/JJ dog/NN ] barked/VBD at/IN [ the/DT cat/NN ]\n",
    "```\n",
    "\n",
    "简单的加缝器：\n",
    "\n",
    "```py\n",
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    ">>> print cp.parse(sentence)\n",
    "(S\n",
    "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
    "  barked/VBD\n",
    "  at/IN\n",
    "  (NP the/DT cat/NN))\n",
    "```\n",
    "\n",
    "### 分块的表示：标记与树状图\n",
    "作为标注和分析之间的中间状态，块结构可以使用标记或树状图来表示。使用最广泛的表示是IOB标记。每个标识符被用3个特殊的块标签之一标注，I(inside),O(outside)或B(begin)。\n",
    "\n",
    "![7-3](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302088.png)\n",
    "\n",
    "Figure 7-3. Tag representation of chunk structures.\n",
    "\n",
    "![7-4](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302090.png)\n",
    "\n",
    "Figure 7-4. Tree representation of chunk structures.\n",
    "\n",
    "## 开发和评估分块器\n",
    "### 读取IOB格式与CoNLL2000分块语料库\n",
    "```py\n",
    ">>> text = '''\n",
    "... he PRP B-NP\n",
    "... accepted VBD B-VP\n",
    "... the DT B-NP\n",
    "... position NN I-NP\n",
    "... of IN B-PP\n",
    "... vice NN B-NP\n",
    "... chairman NN I-NP\n",
    "... of IN B-PP\n",
    "... Carlyle NNP B-NP\n",
    "... Group NNP I-NP\n",
    "... , , O\n",
    "... a DT B-NP\n",
    "... merchant NN I-NP\n",
    "... banking NN I-NP\n",
    "... concern NN I-NP\n",
    "... . . O\n",
    "... '''\n",
    ">>> nltk.chunk.conllstr2tree(text, chunk_types=['NP']).draw()\n",
    "```\n",
    "\n",
    "![2](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302092.png)\n",
    "\n",
    "读取语料库“训练”部分的100个句子的例子。\n",
    "\n",
    "```py\n",
    ">>> from nltk.corpus import conll2000\n",
    ">>> print conll2000.chunked_sents('train.txt')[99]\n",
    "(S\n",
    "  (PP Over/IN)\n",
    "  (NP a/DT cup/NN)\n",
    "  (PP of/IN)\n",
    "  (NP coffee/NN)\n",
    "  ,/,\n",
    "  (NP Mr./NNP Stone/NNP)\n",
    "  (VP told/VBD)\n",
    "  (NP his/PRP$ story/NN)\n",
    "  ./.)\n",
    "```\n",
    "\n",
    "语料库包含3种分块类型：NP分块，VP分块(has already delivered)和PP分块(because of)。选择NP分块：\n",
    "\n",
    "```py\n",
    ">>> print conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99]\n",
    "(S\n",
    "  Over/IN\n",
    "  (NP a/DT cup/NN)\n",
    "  of/IN\n",
    "  (NP coffee/NN)\n",
    "  ,/,\n",
    "  (NP Mr./NNP Stone/NNP)\n",
    "  told/VBD\n",
    "  (NP his/PRP$ story/NN)\n",
    "  ./.)\n",
    "```\n",
    "\n",
    "### 简单评估和基准\n",
    "以不创建任何块的块分析器cp建立一个baseline。\n",
    "\n",
    "```py\n",
    ">>> from nltk.corpus import conll2000\n",
    ">>> cp = nltk.RegexpParser(\"\")\n",
    ">>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    ">>> print cp.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  43.4%\n",
    "    Precision:      0.0%\n",
    "    Recall:         0.0%\n",
    "    F-Measure:      0.0%\n",
    "```\n",
    "\n",
    "查找以名词短语标记的特征字母（如CD、DT和JJ）开头的标记。\n",
    "\n",
    "```py\n",
    ">>> grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    ">>> cp = nltk.RegexpParser(grammar)\n",
    ">>> print cp.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  87.7%\n",
    "    Precision:     70.6%\n",
    "    Recall:        67.8%\n",
    "    F-Measure:     69.2%\n",
    "```\n",
    "\n",
    "使用unigram标注器对名词短语分块。\n",
    "\n",
    "```py\n",
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    ">>> test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    ">>> train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    ">>> unigram_chunker = UnigramChunker(train_sents)\n",
    ">>> print unigram_chunker.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  92.9%\n",
    "    Precision:     79.9%\n",
    "    Recall:        86.8%\n",
    "    F-Measure:     83.2%\n",
    "\n",
    ">>> postags = sorted(set(pos for sent in train_sents\n",
    "...                      for (word,pos) in sent.leaves()))\n",
    ">>> print unigram_chunker.tagger.tag(postags)\n",
    "[('#', 'B-NP'), ('$', 'B-NP'), (\"''\", 'O'), ('(', 'O'), (')', 'O'),\n",
    " (',', 'O'), ('.', 'O'), (':', 'O'), ('CC', 'O'), ('CD', 'I-NP'),\n",
    " ('DT', 'B-NP'), ('EX', 'B-NP'), ('FW', 'I-NP'), ('IN', 'O'),\n",
    " ('JJ', 'I-NP'), ('JJR', 'B-NP'), ('JJS', 'I-NP'), ('MD', 'O'),\n",
    " ('NN', 'I-NP'), ('NNP', 'I-NP'), ('NNPS', 'I-NP'), ('NNS', 'I-NP'),\n",
    " ('PDT', 'B-NP'), ('POS', 'B-NP'), ('PRP', 'B-NP'), ('PRP$', 'B-NP'),\n",
    " ('RB', 'O'), ('RBR', 'O'), ('RBS', 'B-NP'), ('RP', 'O'), ('SYM', 'O'),\n",
    " ('TO', 'O'), ('UH', 'O'), ('VB', 'O'), ('VBD', 'O'), ('VBG', 'O'),\n",
    " ('VBN', 'O'), ('VBP', 'O'), ('VBZ', 'O'), ('WDT', 'B-NP'),\n",
    " ('WP', 'B-NP'), ('WP$', 'B-NP'), ('WRB', 'O'), ('``', 'O')]\n",
    "```\n",
    "\n",
    "unigram分块器建立完成后，可以很容易建立bigram分块器：\n",
    "\n",
    "```py\n",
    ">>> bigram_chunker = BigramChunker(train_sents)\n",
    ">>> print bigram_chunker.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  93.3%\n",
    "    Precision:     82.3%\n",
    "    Recall:        86.8%\n",
    "    F-Measure:     84.5%\n",
    "```\n",
    "\n",
    "### 训练基于分类器的分块器\n",
    "```py\n",
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train(\n",
    "            train_set, algorithm='megam', trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "\n",
    ">>> def npchunk_features(sentence, i, history):\n",
    "...     word, pos = sentence[i]\n",
    "...     return {\"pos\": pos}\n",
    ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
    ">>> print chunker.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  92.9%\n",
    "    Precision:     79.9%\n",
    "    Recall:        86.7%\n",
    "    F-Measure:     83.2%\n",
    "```\n",
    "\n",
    "添加特征：前面词的词性标记。添加此特征允许分类器模拟相邻标记之间的相互作用。\n",
    "\n",
    "```py\n",
    ">>> def npchunk_features(sentence, i, history):\n",
    "...     word, pos = sentence[i]\n",
    "...     if i == 0:\n",
    "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
    "...     else:\n",
    "...         prevword, prevpos = sentence[i-1]\n",
    "...     return {\"pos\": pos, \"prevpos\": prevpos}\n",
    ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
    ">>> print chunker.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  93.6%\n",
    "    Precision:     81.9%\n",
    "    Recall:        87.1%\n",
    "    F-Measure:     84.4%\n",
    "```\n",
    "\n",
    "为当前词增加特征：\n",
    "\n",
    "```py\n",
    ">>> def npchunk_features(sentence, i, history):\n",
    "...     word, pos = sentence[i]\n",
    "...     if i == 0:\n",
    "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
    "...     else:\n",
    "...         prevword, prevpos = sentence[i-1]\n",
    "...     return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\n",
    ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
    ">>> print chunker.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  94.2%\n",
    "    Precision:     83.4%\n",
    "    Recall:        88.6%\n",
    "    F-Measure:     85.9%\n",
    "```\n",
    "\n",
    "增加更多特征：预取特征、配对功能和复杂的语境特征、tags-since-dt用其创建一个字符串，描述自最近限定词以来遇到的所有词性标记。\n",
    "\n",
    "```py\n",
    ">>> def npchunk_features(sentence, i, history):\n",
    "...     word, pos = sentence[i]\n",
    "...     if i == 0:\n",
    "...         prevword, prevpos = \"<START>\", \"<START>\"\n",
    "...     else:\n",
    "...         prevword, prevpos = sentence[i-1]\n",
    "...     if i == len(sentence)-1:\n",
    "...         nextword, nextpos = \"<END>\", \"<END>\"\n",
    "...     else:\n",
    "...         nextword, nextpos = sentence[i+1]\n",
    "...     return {\"pos\": pos,\n",
    "...             \"word\": word,\n",
    "...             \"prevpos\": prevpos,\n",
    "...             \"nextpos\": nextpos, # 预取特征\n",
    "...             \"prevpos+pos\": \"%s+%s\" % (prevpos, pos),  # 配对功能\n",
    "...             \"pos+nextpos\": \"%s+%s\" % (pos, nextpos),\n",
    "...             \"tags-since-dt\": tags_since_dt(sentence, i)}  # 语境特征\n",
    ">>> def tags_since_dt(sentence, i):\n",
    "...     tags = set()\n",
    "...     for word, pos in sentence[:i]:\n",
    "...         if pos == 'DT':\n",
    "...             tags = set()\n",
    "...         else:\n",
    "...             tags.add(pos)\n",
    "...     return '+'.join(sorted(tags))\n",
    ">>> chunker = ConsecutiveNPChunker(train_sents)\n",
    ">>> print chunker.evaluate(test_sents)\n",
    "ChunkParse score:\n",
    "    IOB Accuracy:  95.9%\n",
    "    Precision:     88.3%\n",
    "    Recall:        90.7%\n",
    "    F-Measure:     89.5%\n",
    "```\n",
    "\n",
    "## 语言结构中的递归\n",
    "### 用级联分块器构建嵌套结构\n",
    "创建一个包含递归规则的多级的分块语法，就可以建立任意深度的分块结构。下例展示了名词短语、介词短语、动词短语和句子的模式。这是一个四级分块语法器，可以用来创建深度最多为4的结构。\n",
    "\n",
    "```py\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    ">>> print cp.parse(sentence)\n",
    "(S\n",
    "  (NP Mary/NN)\n",
    "  saw/VBD\n",
    "  (CLAUSE\n",
    "    (NP the/DT cat/NN)\n",
    "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n",
    "```\n",
    "\n",
    "### 树状图\n",
    "![7-10](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302094.png)\n",
    "\n",
    "## 命名实体识别(Named Entity Recognition)\n",
    "Table 7-3. Commonly used types of named entity\n",
    "\n",
    "| NE type | Examples |\n",
    "| ------- | -------- |\n",
    "| ORGANIZATION | Georgia-Pacific Corp., WHO |\n",
    "| PERSON | Eddy Bonte, President Obama |\n",
    "| LOCATION | Murray River, Mount Everest |\n",
    "| DATE | June, 2008-06-29 |\n",
    "| TIME | two fifty a m, 1:30 p.m. |\n",
    "| MONEY | 175 million Canadian Dollars, GBP 10.40 |\n",
    "| PERCENT | twenty pct, 18.75 % |\n",
    "| FACILITY | Washington Monument, Stonehenge |\n",
    "| GPE | South East Asia, Midlothian |\n",
    "\n",
    "命名实体识别(`named entity recognition`, NER)系统的目标是识别所有文字提及的命名实体。这可以分解成两个子任务：确定NE的边界和确定其类型。\n",
    "\n",
    "![7-5](https://www.safaribooksonline.com/library/view/natural-language-processing/9780596803346/httpatomoreillycomsourceoreillyimages302098.png)\n",
    "\n",
    "Figure 7-5. Location detection by simple lookup for a news story: Looking up every word in a gazetteer is error-prone; case distinctions may help, but these are not always present.\n",
    "\n",
    "NLTK提供了一个已经训练好的可以识别命名实体的分类器（如果设置参数binary=True，那个命名实体只被标注为NE；否则，分类器会添加类型标签，如PERSON、ORGANIZATION和GPE）：\n",
    "\n",
    "```py\n",
    ">>> sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    ">>> print nltk.ne_chunk(sent, binary=True)\n",
    "(S\n",
    "  The/DT\n",
    "  (NE U.S./NNP)\n",
    "  is/VBZ\n",
    "  one/CD\n",
    "  ...\n",
    "  according/VBG\n",
    "  to/TO\n",
    "  (NE Brooke/NNP T./NNP Mossman/NNP)\n",
    "  ...)\n",
    ">>> print nltk.ne_chunk(sent)\n",
    "(S\n",
    "  The/DT\n",
    "  (GPE U.S./NNP)\n",
    "  is/VBZ\n",
    "  one/CD\n",
    "  ...\n",
    "  according/VBG\n",
    "  to/TO\n",
    "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
    "  ...)\n",
    "```\n",
    "\n",
    "## 关系抽取(Relation Extraction)\n",
    "通常会寻找指定类型的命名实体之间的关系。方法之一是首选寻找所有（X，a，Y）形式三元组，其中X和Y是指定类型的命名实体，a表示X和Y之间关系的字符串。然后使用正则表达式从a的实体中抽出正在查找的关系。\n",
    "\n",
    "```py\n",
    ">>> IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    ">>> for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "...     for rel in nltk.sem.extract_rels('ORG', 'LOC', doc,\n",
    "...                                      corpus='ieer', pattern = IN):\n",
    "...         print nltk.sem.show_raw_rtuple(rel)\n",
    "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
    "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
    "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
    "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
    "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
    "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
    "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
    "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
    "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
    "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
    "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
    "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
    "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n",
    "```\n",
    "\n",
    "false positives : [ORG: House Transportation Committee] , secured the most money in the [LOC: New York]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
