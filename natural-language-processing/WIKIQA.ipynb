{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIKIQA: A Challenge Dataset for Open-Domain Question Answering\n",
    "\n",
    "    Yi Yang\n",
    "    Georgia Institute of Technology Atlanta, GA 30308, USA\n",
    "    yiyang@gatech.edu\n",
    "\n",
    "    Wen-tau Yih,Christopher Meek\n",
    "    Microsoft Research\n",
    "    Redmond, WA 98052, USA {scottyih,meek}@microsoft.com\n",
    "\n",
    "http://www.aclweb.org/anthology/D15-1237"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1. Answer sentence selection 相比较其他QA形式更容易做到\n",
    "1. Answer triggering 基于给定Question检测是否存在正确的Answer（从candidate sentences中），如果存在则给出正确的Answer\n",
    "1. WIKIQA Dataset\n",
    "    1. 从Bing日志中清洗出一篇Question\n",
    "    1. 标注：给出Question和一段Paragraph，确认是否存在答案，如果存在\b，则\u001b需要标注出需要的答案（句子）\n",
    "1. 实验\n",
    "    1. Baseline：Question和Answer之间的overlap word count\n",
    "    1. LCLR:利用大量lexical semantic features\n",
    "    1. Paragraph Vector:计算Question和Answer两个Paragraph Vector的cosine值\n",
    "    1. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "**Answer sentence selection** is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences.\n",
    "\n",
    "The following example illustrates an answer that does not share any content words with the question and would not be selected:\n",
    "\n",
    "    Q: How did Seminole war end?\n",
    "    A: Ultimately, the Spanish Crown ceded the colony to United States rule.\n",
    "\n",
    "One significant concern with this approach is that **the lexical overlap will make sentence selection easier** for the QASENT dataset and **might inflate the performance** of existing systems in more natural settings.\n",
    "\n",
    "We address a new challenge of **answer triggering**, an important component in QA systems, where the goal is to detect whether there exist correct answers in the set of candidate sentences for the question, and return a correct answer if there exists such one.\n",
    "\n",
    "## WIKIQA Dataset\n",
    "### Question & Sentence Selection\n",
    "1. used Bing query logs as the question source\n",
    "    1. selected question-like queries using simple heuristics, such as queries starting with a WH-word (e.g., “what” or “how”) and queries ending with a question mark\n",
    "    1. filtered out some entity queries that satisfy the rules, such as the TV show “how I met your mother.”\n",
    "1. selected only the queries issued by at least 5 unique users and have clicks to Wikipedia\n",
    "1. sampled 3,050 questions based on query frequencies\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-18/32101701.jpg)\n",
    "\n",
    "### Sentence Annotation\n",
    "We employed crowdsourcing workers through a platform, which is similar to Amazon MTurk, to label whether the candidate answer sentences of a question are correct.We designed a cascaded Web UI that consists of two stages. \n",
    "1. The first stage shows a testing question, along with the title and the summary paragraph of the associated Wikipedia page, asking the worker “Does the short paragraph answer the question?” If the worker chooses “No”, then equivalently all the sentences in this paragraph are marked incorrect and the UI moves to the next question. \n",
    "1. Otherwise, the system enters the second stage and puts a checkbox along each sentence. The worker is then asked to check the sentences that can answer the question in isolation, assuming coreference is resolved. To ensure the label quality, each question was labeled by three workers.\n",
    "\n",
    "### WIKIQA vs. QASENT\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-18/22992610.jpg)\n",
    "\n",
    "## Experiments\n",
    "### Baseline Systems\n",
    "Two simple word matching methods: Word Count and Weighted Word Count.(counts the number of non-stopwords in the question that also occur in the answer sentence)\n",
    "\n",
    "LCLR (Yih et al., 2013):makes use of rich lexical semantic features, including word/lemma matching, WordNet and vector-space lexical semantic models.\n",
    "\n",
    "Paragraph Vector(PV; Le and Mikolov, 2014):the model score of PV is the cosine similarity score between the question vector and the sentence vector.\n",
    "\n",
    "Convolutional Neural Networks (CNN; Yu et al., 2014):\n",
    "1. employ a bigram CNN model with average pooling\n",
    "1. use the pre-trained word2vec embeddings provided by Mikolov et al. (2013) as model input\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-18/45971991.jpg)\n",
    "\n",
    "### Evaluation of Answer Triggering\n",
    "We propose the answer triggering task, a new challenge for the question answering problem, which requires QA systems to: \n",
    "1. detect whether there is at least one correct answer in the set of candidate sentences for the question;\n",
    "1. if yes, select one of the correct answer sentences from the candidate sentence set.\n",
    "\n",
    "\n",
    "We further study three additional features: the length of question (QLen), the length of sentence (SLen), and the class of the ques- tion (QClass).\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-18/99076227.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
