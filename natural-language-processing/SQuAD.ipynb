{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD: 100,000+ Questions for Machine Comprehension of Text\n",
    "\n",
    "    Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang\n",
    "    {pranavsr,zjian,klopyrev,pliang}@cs.stanford.edu\n",
    "    Computer Science Department Stanford University\n",
    "\n",
    "https://arxiv.org/pdf/1606.05250.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "1. 数据生成\n",
    "    1. 选择一个很好的文章源，备选10000篇文章，随机选出536篇，并进行清洗。\n",
    "    1. 生成问题-答案。通过Mechanical Turk做众包标注。\b必须是native\b speaker。\n",
    "        1. 方案一，在每一段落上创作5个问题，答案要在段落里标注出来。\n",
    "        1. 方案二，\u001b\u001b提供问题让标注人员作答。如果标注人员无法作答，标注人员被要求提交问题（但\u001b无需作答）。\n",
    "1. 模型\n",
    "    1. baseline\n",
    "        1. 算法query和answer之间overlap\n",
    "        1. 算法query和answer之间distance\n",
    "    1. 逻辑回归\n",
    "        1. 180 million features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "SQuAD contains 107,785 question-answer pairs on 536 articles.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-17/32513290.jpg)\n",
    "\n",
    "## Dataset Collection\n",
    "To retrieve high-quality articles, we used Project Nayuki’s Wikipedia’s internal PageRanks to obtain the top 10000 articles of English Wikipedia, from which we sampled 536 articles uniformly at random.\n",
    "\n",
    "With Amazon Mechanical Turk as its backend, crowdworkers were required to have a 97% HIT acceptance rate, a minimum of 1000 HITs, and be located in the United States or Canada.\n",
    "\n",
    "On each paragraph, crowdworkers were tasked with asking and answering up to 5 questions on the content of that paragraph. The questions had to be entered in a text field, and the answers had to be highlighted in the paragraph.\n",
    "\n",
    "In the secondary answer generation task, each crowdworker was shown only the questions along with the paragraphs of an article, and asked to select the shortest span in the paragraph that answered the question. If a question was not answerable by a span in the paragraph, workers were asked to submit the question without marking an answer.\n",
    "\n",
    "## Dataset Analysis\n",
    "### Diversity in answers\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-17/98114635.jpg)\n",
    "\n",
    "### Reasoning required to answer questions\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-17/65530306.jpg)\n",
    "\n",
    "### Stratification by syntactic divergence\n",
    "We also develop an automatic method to quantify the syntactic divergence between a question and the sentence containing the answer.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-17/34026442.jpg)\n",
    "\n",
    "The histogram in Figure 4a shows that there is a wide range of syntactic divergence in our dataset.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-17/70803671.jpg)\n",
    "\n",
    "## Methods\n",
    "### Sliding Window Baseline\n",
    "For each candidate answer, we compute the unigram/bigram overlap between the sentence containing it (excluding the candidate itself) and the question. We keep all the candidates that have the maximal overlap. Among these, we select the best one using the sliding-window approach proposed in Richardson et al. (2013).\n",
    "\n",
    "In addition to the basic sliding window approach, we also implemented the distance-based extension (Richardson et al., 2013).\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-17/23749228.jpg)\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "We discretize each continuous feature into 10 equallysized buckets, building a total of 180 million features, most of which are lexicalized features or dependency tree path features. The descriptions and examples of the features are summarized in Table 4.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-1-17/39837132.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
