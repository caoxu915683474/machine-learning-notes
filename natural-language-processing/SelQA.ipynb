{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelQA: A New Benchmark for Selection-based Question Answering\n",
    "    Tomasz Jurczyk,Michael Zhai,Jinho D. Choi\n",
    "    Mathematics and Computer Science Emory University\n",
    "    Atlanta, GA 30322, USA\n",
    "\n",
    "https://arxiv.org/pdf/1606.08513.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "针对Selection-based question answering两类（answer sentence selection和answer triggering）。\n",
    "\n",
    "提出了一种系统化的标注方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "**Selection-based question answering** is the task of selecting a segment of text, or interchangeably a context, from a provided set of contexts that best answers a posed question.\n",
    "\n",
    "Selection-based question answering is subdivided into **answer sentence selection** and **answer triggering**.\n",
    "\n",
    "- **Answer sentence selection** is defined as ranking sentences that answer a question higher than the irrelevant sentences where there is at least a single sentence that answers the question in a provided set of candidate sentences. \n",
    "- **Answer triggering** is defined as selecting any number (n >= 0) of sentences from a set of candidate sentences that answers a question where the set of candidate sentences may or may not contain sentences that answer the question. \n",
    "\n",
    "## CORPUS\n",
    "### A. Data Collection\n",
    "A total of 486 articles are uniformly sampled from the following 10 topics of the English Wikipedia, dumped on August, 2014:\n",
    "\n",
    "    Arts, Country, Food, Historical Events, Movies, Music, Science, Sports, Travel, TV.\n",
    "\n",
    "TABLE I LEXICAL STATISTICS OF OUR CORPUS.\n",
    "  \n",
    "Type | Count\n",
    "--- | ---\n",
    "Total # of articles | 486\n",
    "Total # of sections | 8,481\n",
    "Total # of sentences | 113,709\n",
    "Total # of tokens | 2,810,228\n",
    "\n",
    "### B. Annotation Scheme\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-2-6/92998980.jpg)\n",
    "\n",
    "1. **Task 1**:Approximately two thousand sections are randomly selected from the 486 articles in Section III-A. All the selected sections consist of 3 to 25 sentences; we found that annotators experienced difficulties accurately and timely annotating longer sections. For each section, annotators are instructed to generate a question that can be answered in one or more sentences in the provided section, and select the corresponding sentence or sentences that answer the question. The annotators are provided with the instructions, the topic, the article title, the section title, and the list of numbered sentences in the section (Table II).\n",
    "1. **Task 2**:Annotators are asked to create another set of ≈2K questions from the same selected sections excluding the sentences selected as answers in Task 1. The goal of Task 2 is to generate questions that can be answered from sentences different from those used to answer questions generated in the Task 1. The annotators are provided with the same information as in Task 1, except that the sentences used as the answer contexts in Task 1 are crossed out (line 1 in Table II). Annotators are instructed not to use these sentences to generate new questions.\n",
    "1. **Task 3**:Although our instruction encourages the annotators to create questions in their own words, annotators will generate questions with some lexical overlap with the corresponding contexts. The intention of this task is to mitigate the effects of annotators’ tendency to generating questions with similar vocabulary and phrasing to answer contexts. This is a necessary step in creating a corpus that evaluates reading comprehension rather than ability to model word co-occurrences. The annotators are provided with the previously generated questions and answer contexts and are instructed to paraphrase these questions using different terms.\n",
    "1. **Task 4**:Most questions generated by Tasks 1-3 are of high quality, that is they can be answered by a human when given the corresponding contexts; however, there are some questions that are ambiguous in meaning and difficult for humans to answer correctly. These difficult questions often incorrectly assume that the related sections are provided with the questions. For instance, it is impossible to answer the question from Task 3.1 in Table II unless the related section is provided with the question. These ambiguous questions are sent back to the annotators for revision.\n",
    "1. **Task 5**:To generate answer contexts for answer triggering, all 14M sections from the entire English Wikipedia are indexed, and each question from Tasks 1-4 is queried. Every sentence in the top 5 highest scoring sections from Elasticsearch are collected as candidates, which may or may not include the answer context that resolves the question.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-2-6/24016687.jpg)\n",
    "\n",
    "### C. Corpus Analysis\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-2-6/56423240.jpg)\n",
    "\n",
    "## SYSTEMS\n",
    "First, a convolutional layer is applied on the image of text using the hyperbolic tangent activation function. The image consists of rows standing for consecutive words in two sentences, the question (q) and the answer candidate (a), where the words are represented by their embeddings.\n",
    "\n",
    "Next, we use a logistic regression model, where the CNN score from the output layer is used as one of the features.\n",
    "\n",
    "Other features in the logistic regression are the number of overlapping words between q and a, say Ω, Ω normalized by the IDF, and the question length.\n",
    "\n",
    "![](http://ou8qjsj0m.bkt.clouddn.com//18-2-6/19387992.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
