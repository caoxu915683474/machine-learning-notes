{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization: A Simple Tutorial and Implementation in Python\n",
    "\n",
    "http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/\n",
    "\n",
    "## Basic Ideas\n",
    "From an application point of view, matrix factorization can be used to discover latent features underlying the interactions between two different kinds of entities.\n",
    "\n",
    "In a recommendation system such as Netflix or MovieLens, there is a group of `users` and a set of `items` (movies for the above two systems). Given that each users have rated some items in the system, we would like to predict how the users would rate the items that they have not yet rated, such that we can make recommendations to the users.\n",
    "\n",
    "Assume now we have 5 users and 4 items, and ratings are integers ranging from 1 to 5:\n",
    "\n",
    "| item/user | D1 | D2 | D3 | D4 |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| U1 | 5 | 3 | - | 1 |\n",
    "| U2 | 4 | - | - | 1 |\n",
    "| U3 | 1 | 1 | - | 5 |\n",
    "| U4 | 1 | - | - | 4 |\n",
    "| U5 | - | 1 | 5 | 4 |\n",
    "\n",
    "Hence, the task of predicting the missing ratings can be considered as filling in the blanks.\n",
    "\n",
    "## The mathematics of matrix factorization\n",
    "\n",
    "- Firstly, we have a set U of users, and a set D of items. Let R of size |U|x|D| be the matrix that contains all the ratings that the users have assigned to the items.\n",
    "- Also, we assume that we would like to discover K latent features. Our task, then, is to find two matrics matrices P (a |U|xK matrix) and Q(a |D|xK matrix) such that their product approximates R: \n",
    "\n",
    "$R\\approx P \\times Q^T = \\hat{R}$\n",
    "\n",
    "- Each row of P would represent the strength of the associations between a user and the features.\n",
    "- Each row of Q would represent the strength of the associations between an item and the features.\n",
    "- To get the prediction of a rating of an item $d_j$ by $u_i$, we can calculate the dot product of the two vectors corresponding to $u_i$ and $d_j$:\n",
    "\n",
    "$\\hat{r_{ij}} = p_i^Tq_j = \\sum_{k=1}^k p_{ik}q_{kj}$\n",
    "\n",
    "Error estimate:\n",
    "\n",
    "$e_{ij}^2 = (r_{ij} - \\hat{r_{ij}})^2 = (r_{ij} - \\sum_{k=1}^K p_{ik}q_{kj})^2$\n",
    "\n",
    "To minimize the error, we have to know in which direction we have to modify the values of $p_{ik}$ and $q_{kj}$:\n",
    "\n",
    "$\\frac{\\partial}{\\partial{p_{ik}}}e_{ij}^2 = -2(r_{ij}-\\hat{r_{ij}})(q_{kj}) = -2e_{ij}q_{kj}$\n",
    "\n",
    "$\\frac{\\partial}{\\partial{q_{ik}}}e_{ij}^2 = -2(r_{ij}-\\hat{r_{ij}})(p_{ik}) = -2e_{ij}p_{ik}$\n",
    "\n",
    "Having obtained the gradient, we can now formulate the update rules for both $p_{ik}$ and $q_{kj}$:\n",
    "\n",
    "$p_{ik}' = p_{ik} + \\alpha \\frac{\\partial}{\\partial{p_{ik}}}e_{ij}^2 = p_{ik} + 2\\alpha e_{ij}q_{kj}$\n",
    "\n",
    "$q_{kj}' = q_{kj} + \\alpha \\frac{\\partial}{\\partial{p_{kj}}}e_{ij}^2 = q_{kj} + 2\\alpha e_{ij}q_{ik}$\n",
    "\n",
    "$\\alpha$:learning rate\n",
    "\n",
    "We can then iteratively perform the operation until the error converges to its minimum. We can check the overall error:\n",
    "\n",
    "$E = \\sum_{(u_i,d_j,r_{ij}) \\in T}e_{ij} = \\sum_{(u_i,d_j,r_{ij}) \\in T}(r_{ij} - \\sum_{k=1}^K p_{ik}q_{kj})^2$\n",
    "\n",
    "## Regularization\n",
    "A common extension to this basic algorithm is to introduce regularization to avoid overfitting:\n",
    "\n",
    "$e_{ij}^2 = (r_{ij} - \\hat{r_{ij}})^2 = (r_{ij} - \\sum_{k=1}^K p_{ik}q_{kj})^2 + \\frac{\\beta}{2}\\sum_{k=1}^K(\\|P\\|^2 + \\|Q\\|^2)$\n",
    "\n",
    "$\\beta$ is used to control the magnitudes of the user-feature and item-feature vectors such that P and Q would give a good approximation of R without having to contain large numbers.\n",
    "\n",
    "The new update rules are as follows:\n",
    "\n",
    "$p_{ik}' = p_{ik} + \\alpha \\frac{\\partial}{\\partial{p_{ik}}}e_{ij}^2 = p_{ik} + \\alpha(2e_{ij}q_{kj} - \\beta p_{ik})$\n",
    "\n",
    "$q_{kj}' = q_{kj} + \\alpha \\frac{\\partial}{\\partial{p_{kj}}}e_{ij}^2 = q_{kj} + \\alpha(2e_{ij}q_{ik} - \\beta q_{kj})$\n",
    "\n",
    "## Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "@INPUT:\n",
    "    R     : a matrix to be factorized, dimension N x M\n",
    "    P     : an initial matrix of dimension N x K\n",
    "    Q     : an initial matrix of dimension M x K\n",
    "    K     : the number of latent features\n",
    "    steps : the maximum number of steps to perform the optimisation\n",
    "    alpha : the learning rate\n",
    "    beta  : the regularization parameter\n",
    "@OUTPUT:\n",
    "    the final matrices P and Q\n",
    "\"\"\"\n",
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "    Q = Q.T\n",
    "    for step in xrange(steps):\n",
    "        for i in xrange(len(R)):\n",
    "            for j in xrange(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    eij = R[i][j] - np.dot(P[i, :], Q[:, j])\n",
    "                    for k in xrange(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\n",
    "        e = 0\n",
    "        for i in xrange(len(R)):\n",
    "            for j in xrange(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    e += pow(R[i][j] - np.dot(P[i, :], Q[:, j]), 2)\n",
    "                    for k in xrange(K):\n",
    "                        e += (beta/2) * (pow(P[i][k], 2) + pow(Q[k][j], 2))\n",
    "        if e < 0.001:\n",
    "            break\n",
    "    return P, Q.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.06764261  2.72043466  5.83556815  0.99861472]\n",
      " [ 3.91441045  2.10130191  4.60677696  1.00374907]\n",
      " [ 1.13440717  0.60796713  3.33152155  4.96801169]\n",
      " [ 0.93310611  0.50010531  2.69581378  3.98212046]\n",
      " [ 2.91771876  1.56557017  4.82846645  4.01547552]]\n"
     ]
    }
   ],
   "source": [
    "R = [\n",
    "     [5,3,0,1],\n",
    "     [4,0,0,1],\n",
    "     [1,1,0,5],\n",
    "     [1,0,0,4],\n",
    "     [0,1,5,4],\n",
    "    ]\n",
    "\n",
    "R = np.array(R)\n",
    "\n",
    "N = len(R)\n",
    "M = len(R[0])\n",
    "K = 2\n",
    "\n",
    "P = np.random.rand(N, K)  # shape (5, 2)\n",
    "Q = np.random.rand(M, K)  # shape (4, 2)\n",
    "\n",
    "nP, nQ = matrix_factorization(R, P, Q, K)\n",
    "print np.dot(nP, nQ.T)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
